{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import Linear, ReLU, BCEWithLogitsLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, Dropout\n",
    "from torch.optim import Adam,SGD\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Subset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.data.to_string(index=False)\n",
    "#         self.transform = transform\n",
    "#         self.data['Super Class'] = self.data['Super Class'].apply(lambda x: 0 if x == 'benign' else 1)\n",
    "#         label_map = {\n",
    "#             'adenosis': 0,\n",
    "#             'fibroadenoma': 1,\n",
    "#             'tubular_adenoma': 2,\n",
    "#             'phyllodes_tumor': 3,\n",
    "#             'ductal_carcinoma': 4,\n",
    "#             'lobular_carcinoma': 5,\n",
    "#             'mucinous_carcinoma': 6,\n",
    "#             'papillary_carcinoma': 7\n",
    "#         }\n",
    "#         self.data['Sub Class'] = self.data['Sub Class'].map(label_map)\n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_path = self.data.iloc[idx, 0]\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         SuperClass = self.data.iloc[idx, 1]\n",
    "#         # SuperClass = torch.tensor(SuperClass, dtype=torch.long)\n",
    "#         SubClass = self.data.iloc[idx, 3]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         image = asarray(image)\n",
    "#         return (image, SuperClass, SubClass)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256,256)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# dataset = ImageDataset(csv_file='csv windows.csv', transform=transform)\n",
    "# images = [x[0] for x in dataset]\n",
    "# superclass_labels = [x[1] for x in dataset]\n",
    "# subclass_labels = [x[2] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Serialize and save the list to a file\n",
    "# with open('image_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(images, f)\n",
    "\n",
    "# with open('superclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(superclass_labels, f)\n",
    "    \n",
    "\n",
    "# with open('subclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(subclass_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list back into memory\n",
    "with open('image_list.pkl', 'rb') as f:\n",
    "    images = pickle.load(f)\n",
    "    \n",
    "with open('superclass_labels.pkl', 'rb') as f:\n",
    "    superclass_labels = pickle.load(f)\n",
    "\n",
    "with open('subclass_labels.pkl', 'rb') as f:\n",
    "    subclass_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(images, superclass_labels, subclass_labels, test_size=0.3, stratify=superclass_labels, random_state=42)\n",
    "x_train, x_val, y_train, y_val, z_train, z_val = train_test_split(x_train, y_train, z_train, test_size=0.3, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Train=226, Val=107, Test=111\n",
      "Class 1: Train=484, Val=211, Test=319\n",
      "Class 2: Train=282, Val=111, Test=176\n",
      "Class 3: Train=223, Val=92, Test=138\n",
      "Class 4: Train=1700, Val=737, Test=1014\n",
      "Class 5: Train=284, Val=128, Test=214\n",
      "Class 6: Train=408, Val=175, Test=209\n",
      "Class 7: Train=268, Val=100, Test=192\n"
     ]
    }
   ],
   "source": [
    "# get the unique labels in y\n",
    "unique_labels = np.unique(subclass_labels)\n",
    "\n",
    "# count the number of samples in each set for each class\n",
    "train_counts = [np.sum(z_train == label) for label in unique_labels]\n",
    "val_counts = [np.sum(z_val == label) for label in unique_labels]\n",
    "test_counts = [np.sum(z_test == label) for label in unique_labels]\n",
    "\n",
    "# print the counts for each class in each set\n",
    "for i, label in enumerate(unique_labels): \n",
    "    print(f\"Class {label}: Train={train_counts[i]}, Val={val_counts[i]}, Test={test_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.z[index]\n",
    "\n",
    "train_dataset = YourDataset(x_train, y_train, z_train)\n",
    "val_dataset = YourDataset(x_val, y_val, z_val)\n",
    "test_dataset = YourDataset(x_test, y_test, z_test)\n",
    "\n",
    "subclass_labels = torch.tensor(z_train)\n",
    "subclasses = torch.unique(subclass_labels)\n",
    "subclass_freqs = []\n",
    "\n",
    "for subclass in subclasses:\n",
    "    subclass_counts = sum(subclass_labels == subclass)\n",
    "    subclass_freqs.append(1/subclass_counts)\n",
    "\n",
    "subclass_weights = torch.zeros_like(subclass_labels).float()\n",
    "\n",
    "for idx, label in enumerate(subclass_labels):\n",
    "    subclass_weights[idx] = subclass_freqs[int(label)]\n",
    "\n",
    "# # Create a sampler to handle imbalanced data\n",
    "# class_counts = np.bincount(y_train)\n",
    "# weights = 1 / class_counts[y_train]\n",
    "# weights = torch.FloatTensor(weights)\n",
    "sampler = WeightedRandomSampler(subclass_weights, len(subclass_weights))\n",
    "\n",
    "# # Create a sampler for validation data\n",
    "# val_sampler = torch.utils.data.sampler.SequentialSampler(val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('RadImageNet-ResNet50_notop.h5')\n",
    "keras_weights = model.get_weights()\n",
    "# print(keras_weights)\n",
    "\n",
    "layer_names = []\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.BatchNormalization)):\n",
    "          layer_names.append(layer.name)\n",
    "\n",
    "        #   print(layer.name)\n",
    "# print(layer_names)\n",
    "\n",
    "# Loop through the layers and create a dictionary mapping layer names to their associated weights\n",
    "weights_dict = {layer.name: layer.get_weights() for layer in model.layers if isinstance(layer, tf.keras.layers.Layer) and layer.weights}\n",
    "\n",
    "# Print the weights_dict to check its contents\n",
    "# print(weights_dict)\n",
    "\n",
    "\n",
    "new_weights = []\n",
    "new_layer_order = ['conv1_conv', 'conv1_bn', 'conv2_block1_1_conv', 'conv2_block1_1_bn', 'conv2_block1_2_conv',\n",
    "'conv2_block1_2_bn','conv2_block1_3_conv','conv2_block1_3_bn','conv2_block1_0_conv','conv2_block1_0_bn',\n",
    "'conv2_block2_1_conv',\n",
    "'conv2_block2_1_bn',\n",
    "'conv2_block2_2_conv',\n",
    "'conv2_block2_2_bn',\n",
    "'conv2_block2_3_conv',\n",
    "'conv2_block2_3_bn',\n",
    "'conv2_block3_1_conv',\n",
    "'conv2_block3_1_bn',\n",
    "'conv2_block3_2_conv',\n",
    "'conv2_block3_2_bn',\n",
    "'conv2_block3_3_conv',\n",
    "'conv2_block3_3_bn',\n",
    "\n",
    "'conv3_block1_1_conv',\n",
    "'conv3_block1_1_bn',\n",
    "'conv3_block1_2_conv',\n",
    "'conv3_block1_2_bn',\n",
    "'conv3_block1_3_conv',\n",
    "'conv3_block1_3_bn',\n",
    "'conv3_block1_0_conv',\n",
    "'conv3_block1_0_bn',\n",
    "'conv3_block2_1_conv',\n",
    "'conv3_block2_1_bn',\n",
    "'conv3_block2_2_conv',\n",
    "'conv3_block2_2_bn',\n",
    "'conv3_block2_3_conv',\n",
    "'conv3_block2_3_bn',\n",
    "'conv3_block3_1_conv',\n",
    "'conv3_block3_1_bn',\n",
    "'conv3_block3_2_conv',\n",
    "'conv3_block3_2_bn',\n",
    "'conv3_block3_3_conv',\n",
    "'conv3_block3_3_bn',\n",
    "'conv3_block4_1_conv',\n",
    "'conv3_block4_1_bn',\n",
    "'conv3_block4_2_conv',\n",
    "'conv3_block4_2_bn',\n",
    "'conv3_block4_3_conv',\n",
    "'conv3_block4_3_bn',\n",
    "\n",
    "'conv4_block1_1_conv',\n",
    "'conv4_block1_1_bn',\n",
    "'conv4_block1_2_conv',\n",
    "'conv4_block1_2_bn',\n",
    "'conv4_block1_3_conv',\n",
    "'conv4_block1_3_bn',\n",
    "'conv4_block1_0_conv',\n",
    "'conv4_block1_0_bn',\n",
    "'conv4_block2_1_conv',\n",
    "'conv4_block2_1_bn',\n",
    "'conv4_block2_2_conv',\n",
    "'conv4_block2_2_bn',\n",
    "'conv4_block2_3_conv',\n",
    "'conv4_block2_3_bn',\n",
    "'conv4_block3_1_conv',\n",
    "'conv4_block3_1_bn',\n",
    "'conv4_block3_2_conv',\n",
    "'conv4_block3_2_bn',\n",
    "'conv4_block3_3_conv',\n",
    "'conv4_block3_3_bn',\n",
    "'conv4_block4_1_conv',\n",
    "'conv4_block4_1_bn',\n",
    "'conv4_block4_2_conv',\n",
    "'conv4_block4_2_bn',\n",
    "'conv4_block4_3_conv',\n",
    "'conv4_block4_3_bn',\n",
    "'conv4_block5_1_conv',\n",
    "'conv4_block5_1_bn',\n",
    "'conv4_block5_2_conv',\n",
    "'conv4_block5_2_bn',\n",
    "'conv4_block5_3_conv',\n",
    "'conv4_block5_3_bn',\n",
    "'conv4_block6_1_conv',\n",
    "'conv4_block6_1_bn',\n",
    "'conv4_block6_2_conv',\n",
    "'conv4_block6_2_bn',\n",
    "'conv4_block6_3_conv',\n",
    "'conv4_block6_3_bn',\n",
    "\n",
    "'conv5_block1_1_conv',\n",
    "'conv5_block1_1_bn',\n",
    "'conv5_block1_2_conv',\n",
    "'conv5_block1_2_bn',\n",
    "'conv5_block1_3_conv',\n",
    "'conv5_block1_3_bn',\n",
    "'conv5_block1_0_conv',\n",
    "'conv5_block1_0_bn',\n",
    "'conv5_block2_1_conv',\n",
    "'conv5_block2_1_bn',\n",
    "'conv5_block2_2_conv',\n",
    "'conv5_block2_2_bn',\n",
    "'conv5_block2_3_conv',\n",
    "'conv5_block2_3_bn',\n",
    "'conv5_block3_1_conv',\n",
    "'conv5_block3_1_bn',\n",
    "'conv5_block3_2_conv',\n",
    "'conv5_block3_2_bn',\n",
    "'conv5_block3_3_conv', 'conv5_block3_3_bn']\n",
    "\n",
    "# Rearrange the weights according to the new layer order\n",
    "for layer_name in new_layer_order:\n",
    "    weight = weights_dict[layer_name]\n",
    "    # print(weight)\n",
    "    new_weights.append(weight)\n",
    "\n",
    "# print(new_weights[1][0])\n",
    "#remove bias of convolutional layer\n",
    "new_weights_wo_convbias = []\n",
    "\n",
    "for i, weight in enumerate(new_weights):\n",
    "    if i % 6 == 1:\n",
    "        continue  # Skip the indices to neglect\n",
    "    new_weights_wo_convbias.append(weight)\n",
    "\n",
    "# print(new_weights_wo_convbias)\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the weights\n",
    "keras_weights = new_weights\n",
    "# print(keras_weights)\n",
    "# Load the PyTorch model\n",
    "resnet_model = torchvision.models.resnet50(weights=True) #implemented based on the previous model (by myself)\n",
    "new_state_dict = {}\n",
    "with torch.no_grad():\n",
    "    y = 1\n",
    "    for i, layer in enumerate(resnet_model.modules()):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            # extract the weights and biases from the TensorFlow weights\n",
    "            weight_tf_conv = keras_weights[(2*y)-2][0]\n",
    "            # print(weight_tf_conv)\n",
    "            \n",
    "            layer.weight.data = torch.tensor(weight_tf_conv.transpose(), dtype=torch.float)\n",
    "            # print(layer.weight.data)\n",
    "            # print(layer)\n",
    "\n",
    "        if isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            weights_tf_batch = keras_weights[(2*y)-1][0]\n",
    "            bias_tf_batch = keras_weights[(2*y)-1][1]\n",
    "            layer.weight.data= torch.tensor(weights_tf_batch, dtype=torch.float)\n",
    "            layer.bias.data = torch.tensor(bias_tf_batch, dtype=torch.float)\n",
    "            y = y+1\n",
    "\n",
    "\n",
    "resnet_model.load_state_dict(new_state_dict, strict=False)\n",
    "# Print the PyTorch model layer name and its pre-trained weights\n",
    "# for name, param in resnet_model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(param)\n",
    "\n",
    "lt=10\n",
    "cntr = 0\n",
    "for child in resnet_model.children():\n",
    "    cntr+=1\n",
    "\n",
    "    if cntr < lt:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = torch.nn.Linear(in_features = num_ftrs, out_features = 1, bias=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# optimizer and loss function\n",
    "\n",
    "# define cnn model\n",
    "model =  resnet_model.to(device)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# define loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train(epoch):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tr_loss=0\n",
    "    model.train()\n",
    "    \n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, subclass = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        # print(labels)\n",
    "        # print(subclass)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # predcition for training set\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        \n",
    "        outputs = outputs.flatten()\n",
    "        total += labels.size(0)\n",
    "        # print(total)\n",
    "        \n",
    "        y_2 = torch.zeros(len(outputs))\n",
    "        y_2[outputs>=0.0] = 1\n",
    "        y_2= y_2.int()\n",
    "\n",
    "        y_2 = y_2.to(device)\n",
    "        \n",
    "        for subclasses in range(num_subclasses):\n",
    "            subclass_idx = subclass == subclasses\n",
    "            num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "            subgroup_correct[subclasses] += ((y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                torch.float).sum().item())\n",
    "            \n",
    "        subgroup_accuracy = subgroup_correct / num_samples\n",
    "        # print(y_2)\n",
    "        correct += (y_2 == labels).sum().item()\n",
    "        train_accuracy = correct / total \n",
    "        # for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "        #         confusion_matrix[t.long(), p.long()] += 1\n",
    "        \n",
    "        # clearing the gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute loss\n",
    "        q = torch.tensor([])\n",
    "        eta = 0.01\n",
    "        normalize_loss=False\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        if len(q) == 0:\n",
    "            q = torch.ones(num_subclasses).to(device)\n",
    "            q /= q.sum()\n",
    "\n",
    "        losses = torch.zeros(num_subclasses).to(device)\n",
    "\n",
    "        subclass_counts = torch.zeros(num_subclasses).to(device)\n",
    "        \n",
    "        # computes gDRO loss\n",
    "        # get relative frequency of samples in each subclass\n",
    "        for subclasses in range(num_subclasses):\n",
    "            subclass_idx = subclass == subclasses\n",
    "            subclass_counts[subclasses] = torch.sum(subclass_idx)\n",
    "\n",
    "            # only compute loss if there are actually samples in that class\n",
    "            if torch.sum(subclass_idx) > 0:\n",
    "                losses[subclasses] = criterion(outputs[subclass_idx], labels[subclass_idx])\n",
    "\n",
    "        # update q\n",
    "        if model.training:\n",
    "            q *= torch.exp(eta * losses.data)\n",
    "            q /= q.sum()\n",
    "\n",
    "        if normalize_loss:\n",
    "            losses *= subclass_counts\n",
    "            loss = torch.dot(losses, q)\n",
    "            loss /= batch_size\n",
    "            loss *= num_subclasses\n",
    "        else:\n",
    "            loss = torch.dot(losses, q)\n",
    "        # print(loss)\n",
    "        # compute updates weights of all the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss value\n",
    "    train_loss_value = tr_loss/len(train_loader) \n",
    "    print(\"Epoch: {:.3f}, Loss: {:.3f}, Train_Accuracy: {:.3f}\".format(epoch+1, train_loss_value, train_accuracy)) \n",
    "    # print('confusion matrix of training images: {}'.format(confusion_matrix))\n",
    "\n",
    "    # print(\"Train Accuracy:\", accuracy, \"\\nTrain Accuracy over subgroups:\", subgroup_accuracy, \"\\nTrain Worst Group Accuracy:\",\n",
    "    #           min(subgroup_accuracy))\n",
    "    \n",
    "    return train_loss_value, train_accuracy, subgroup_accuracy \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            # print(labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            # loss_train = criterion(outputs, labels)\n",
    "            # tr_loss += loss_train.item()\n",
    "            val_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            \n",
    "        print(\"Epoch: {:.3f},  Val Accuracy: {:.3f}\".format(epoch+1,  val_accuracy)) \n",
    "        # print('confusion matrix of validation images: {}'.format(confusion_matrix)) \n",
    "        # print(\"Val Accuracy:\", accuracy, \"\\nVal Accuracy over subgroups:\", subgroup_accuracy, \"\\nVal Worst Group Accuracy:\",\n",
    "        #       min(subgroup_accuracy))       \n",
    "    \n",
    "        return val_accuracy, subgroup_accuracy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        num_subclasses = 8\n",
    "        num_samples = np.zeros(num_subclasses)\n",
    "        subgroup_correct = np.zeros(num_subclasses)\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # print(labels)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            # loss_train = criterion(outputs, labels)\n",
    "            # tr_loss += loss_train.item()\n",
    "            # test_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            # print(y_2)\n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            \n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            test_accuracy = correct / total\n",
    "            \n",
    "        print(\"Epoch: {:.3f}, Test Accuracy: {:.3f}\".format(epoch+1, test_accuracy)) \n",
    "        # print('confusion matrix of testing images: {}'.format(confusion_matrix))\n",
    "        print(\"Test Accuracy:\", test_accuracy, \"\\nTest Accuracy over subgroups:\", subgroup_accuracy, \"\\nTest Worst Group Accuracy:\",\n",
    "              min(subgroup_accuracy)) \n",
    "   \n",
    "        return test_accuracy, subgroup_accuracy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\2278659678.py:44: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.521, Train_Accuracy: 0.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\3435729575.py:45: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000,  Val Accuracy: 0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1642912098.py:47: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Test Accuracy: 0.821\n",
      "Test Accuracy: 0.8209018120522545 \n",
      "Test Accuracy over subgroups: [0.61261261 0.56112853 0.36363636 0.63768116 0.96942801 0.95794393\n",
      " 0.88516746 0.91666667] \n",
      "Test Worst Group Accuracy: 0.36363636363636365\n",
      "Epoch: 2.000, Loss: 0.435, Train_Accuracy: 0.834\n",
      "Epoch: 2.000,  Val Accuracy: 0.810\n",
      "Epoch: 2.000, Test Accuracy: 0.815\n",
      "Test Accuracy: 0.8154235145385588 \n",
      "Test Accuracy over subgroups: [0.77477477 0.60815047 0.45454545 0.66666667 0.94575937 0.92056075\n",
      " 0.83732057 0.79166667] \n",
      "Test Worst Group Accuracy: 0.45454545454545453\n",
      "Epoch: 3.000, Loss: 0.402, Train_Accuracy: 0.843\n",
      "Epoch: 3.000,  Val Accuracy: 0.825\n",
      "Epoch: 3.000, Test Accuracy: 0.831\n",
      "Test Accuracy: 0.831015592077539 \n",
      "Test Accuracy over subgroups: [0.7027027  0.60815047 0.46590909 0.63043478 0.96153846 0.94392523\n",
      " 0.87559809 0.890625  ] \n",
      "Test Worst Group Accuracy: 0.4659090909090909\n",
      "Epoch: 4.000, Loss: 0.383, Train_Accuracy: 0.849\n",
      "Epoch: 4.000,  Val Accuracy: 0.822\n",
      "Epoch: 4.000, Test Accuracy: 0.840\n",
      "Test Accuracy: 0.8398651495996629 \n",
      "Test Accuracy over subgroups: [0.61261261 0.57053292 0.45454545 0.61594203 0.99013807 0.95794393\n",
      " 0.89952153 0.94270833] \n",
      "Test Worst Group Accuracy: 0.45454545454545453\n",
      "Epoch: 5.000, Loss: 0.367, Train_Accuracy: 0.853\n",
      "Epoch: 5.000,  Val Accuracy: 0.804\n",
      "Epoch: 5.000, Test Accuracy: 0.824\n",
      "Test Accuracy: 0.8238516645596292 \n",
      "Test Accuracy over subgroups: [0.5045045  0.50470219 0.38636364 0.52173913 0.99309665 0.97663551\n",
      " 0.93301435 0.97395833] \n",
      "Test Worst Group Accuracy: 0.38636363636363635\n",
      "Epoch: 6.000, Loss: 0.353, Train_Accuracy: 0.857\n",
      "Epoch: 6.000,  Val Accuracy: 0.825\n",
      "Epoch: 6.000, Test Accuracy: 0.833\n",
      "Test Accuracy: 0.8331226295828066 \n",
      "Test Accuracy over subgroups: [0.63963964 0.66144201 0.53409091 0.60144928 0.95759369 0.97663551\n",
      " 0.86124402 0.82291667] \n",
      "Test Worst Group Accuracy: 0.5340909090909091\n",
      "Epoch: 7.000, Loss: 0.342, Train_Accuracy: 0.861\n",
      "Epoch: 7.000,  Val Accuracy: 0.837\n",
      "Epoch: 7.000, Test Accuracy: 0.833\n",
      "Test Accuracy: 0.832701222081753 \n",
      "Test Accuracy over subgroups: [0.7027027  0.64263323 0.52272727 0.63768116 0.9566075  0.97196262\n",
      " 0.84688995 0.82291667] \n",
      "Test Worst Group Accuracy: 0.5227272727272727\n",
      "Epoch: 8.000, Loss: 0.331, Train_Accuracy: 0.866\n",
      "Epoch: 8.000,  Val Accuracy: 0.833\n",
      "Epoch: 8.000, Test Accuracy: 0.829\n",
      "Test Accuracy: 0.8289085545722714 \n",
      "Test Accuracy over subgroups: [0.63963964 0.62695925 0.55681818 0.5942029  0.95759369 0.96728972\n",
      " 0.83253589 0.85416667] \n",
      "Test Worst Group Accuracy: 0.5568181818181818\n",
      "Epoch: 9.000, Loss: 0.320, Train_Accuracy: 0.868\n",
      "Epoch: 9.000,  Val Accuracy: 0.818\n",
      "Epoch: 9.000, Test Accuracy: 0.827\n",
      "Test Accuracy: 0.8272229245680573 \n",
      "Test Accuracy over subgroups: [0.74774775 0.56426332 0.53409091 0.57971014 0.96646943 0.95794393\n",
      " 0.84688995 0.85416667] \n",
      "Test Worst Group Accuracy: 0.5340909090909091\n",
      "Epoch: 10.000, Loss: 0.311, Train_Accuracy: 0.870\n",
      "Epoch: 10.000,  Val Accuracy: 0.843\n",
      "Epoch: 10.000, Test Accuracy: 0.847\n",
      "Test Accuracy: 0.8466076696165191 \n",
      "Test Accuracy over subgroups: [0.79279279 0.68338558 0.65340909 0.67391304 0.94970414 0.95794393\n",
      " 0.80861244 0.82291667] \n",
      "Test Worst Group Accuracy: 0.6534090909090909\n",
      "Epoch: 11.000, Loss: 0.303, Train_Accuracy: 0.873\n",
      "Epoch: 11.000,  Val Accuracy: 0.837\n",
      "Epoch: 11.000, Test Accuracy: 0.834\n",
      "Test Accuracy: 0.8339654445849136 \n",
      "Test Accuracy over subgroups: [0.64864865 0.61442006 0.52840909 0.62318841 0.96942801 0.96261682\n",
      " 0.86602871 0.84375   ] \n",
      "Test Worst Group Accuracy: 0.5284090909090909\n",
      "Epoch: 12.000, Loss: 0.296, Train_Accuracy: 0.876\n",
      "Epoch: 12.000,  Val Accuracy: 0.856\n",
      "Epoch: 12.000, Test Accuracy: 0.861\n",
      "Test Accuracy: 0.8609355246523388 \n",
      "Test Accuracy over subgroups: [0.8018018  0.71159875 0.61931818 0.7173913  0.96646943 0.96261682\n",
      " 0.82296651 0.83854167] \n",
      "Test Worst Group Accuracy: 0.6193181818181818\n",
      "Epoch: 13.000, Loss: 0.290, Train_Accuracy: 0.880\n",
      "Epoch: 13.000,  Val Accuracy: 0.837\n",
      "Epoch: 13.000, Test Accuracy: 0.848\n",
      "Test Accuracy: 0.8482932996207333 \n",
      "Test Accuracy over subgroups: [0.82882883 0.67084639 0.53977273 0.70289855 0.96351085 0.93925234\n",
      " 0.83253589 0.84895833] \n",
      "Test Worst Group Accuracy: 0.5397727272727273\n",
      "Epoch: 14.000, Loss: 0.283, Train_Accuracy: 0.882\n",
      "Epoch: 14.000,  Val Accuracy: 0.856\n",
      "Epoch: 14.000, Test Accuracy: 0.855\n",
      "Test Accuracy: 0.854614412136536 \n",
      "Test Accuracy over subgroups: [0.9009009  0.75548589 0.625      0.75362319 0.95167653 0.91588785\n",
      " 0.74162679 0.81770833] \n",
      "Test Worst Group Accuracy: 0.625\n",
      "Epoch: 15.000, Loss: 0.277, Train_Accuracy: 0.885\n",
      "Epoch: 15.000,  Val Accuracy: 0.837\n",
      "Epoch: 15.000, Test Accuracy: 0.839\n",
      "Test Accuracy: 0.8386009270965024 \n",
      "Test Accuracy over subgroups: [0.86486486 0.68965517 0.51704545 0.7173913  0.94871795 0.92523364\n",
      " 0.77990431 0.83854167] \n",
      "Test Worst Group Accuracy: 0.5170454545454546\n",
      "Epoch: 16.000, Loss: 0.271, Train_Accuracy: 0.887\n",
      "Epoch: 16.000,  Val Accuracy: 0.840\n",
      "Epoch: 16.000, Test Accuracy: 0.842\n",
      "Test Accuracy: 0.8419721871049305 \n",
      "Test Accuracy over subgroups: [0.85585586 0.67084639 0.52272727 0.7173913  0.95364892 0.92523364\n",
      " 0.81818182 0.84375   ] \n",
      "Test Worst Group Accuracy: 0.5227272727272727\n",
      "Epoch: 17.000, Loss: 0.265, Train_Accuracy: 0.888\n",
      "Epoch: 17.000,  Val Accuracy: 0.837\n",
      "Epoch: 17.000, Test Accuracy: 0.839\n",
      "Test Accuracy: 0.8390223345975558 \n",
      "Test Accuracy over subgroups: [0.83783784 0.65203762 0.47727273 0.69565217 0.95956607 0.92523364\n",
      " 0.83253589 0.859375  ] \n",
      "Test Worst Group Accuracy: 0.4772727272727273\n",
      "Epoch: 18.000, Loss: 0.260, Train_Accuracy: 0.892\n",
      "Epoch: 18.000,  Val Accuracy: 0.848\n",
      "Epoch: 18.000, Test Accuracy: 0.851\n",
      "Test Accuracy: 0.8512431521281079 \n",
      "Test Accuracy over subgroups: [0.84684685 0.65830721 0.53977273 0.72463768 0.96548323 0.93457944\n",
      " 0.83732057 0.86979167] \n",
      "Test Worst Group Accuracy: 0.5397727272727273\n",
      "Epoch: 19.000, Loss: 0.254, Train_Accuracy: 0.895\n",
      "Epoch: 19.000,  Val Accuracy: 0.845\n",
      "Epoch: 19.000, Test Accuracy: 0.845\n",
      "Test Accuracy: 0.8445006321112516 \n",
      "Test Accuracy over subgroups: [0.77477477 0.58934169 0.49431818 0.68115942 0.96844181 0.97196262\n",
      " 0.88516746 0.90625   ] \n",
      "Test Worst Group Accuracy: 0.4943181818181818\n",
      "Epoch: 20.000, Loss: 0.249, Train_Accuracy: 0.899\n",
      "Epoch: 20.000,  Val Accuracy: 0.864\n",
      "Epoch: 20.000, Test Accuracy: 0.861\n",
      "Test Accuracy: 0.8613569321533924 \n",
      "Test Accuracy over subgroups: [0.83783784 0.70846395 0.625      0.74637681 0.95857988 0.95794393\n",
      " 0.84210526 0.828125  ] \n",
      "Test Worst Group Accuracy: 0.625\n",
      "Epoch: 21.000, Loss: 0.245, Train_Accuracy: 0.898\n",
      "Epoch: 21.000,  Val Accuracy: 0.868\n",
      "Epoch: 21.000, Test Accuracy: 0.869\n",
      "Test Accuracy: 0.8693636746734091 \n",
      "Test Accuracy over subgroups: [0.8018018  0.73354232 0.63636364 0.73188406 0.96745562 0.95794393\n",
      " 0.84688995 0.85416667] \n",
      "Test Worst Group Accuracy: 0.6363636363636364\n",
      "Epoch: 22.000, Loss: 0.241, Train_Accuracy: 0.901\n",
      "Epoch: 22.000,  Val Accuracy: 0.867\n",
      "Epoch: 22.000, Test Accuracy: 0.870\n",
      "Test Accuracy: 0.8702064896755162 \n",
      "Test Accuracy over subgroups: [0.81081081 0.73981191 0.67045455 0.73188406 0.96252465 0.95794393\n",
      " 0.84688995 0.84375   ] \n",
      "Test Worst Group Accuracy: 0.6704545454545454\n",
      "Epoch: 23.000, Loss: 0.237, Train_Accuracy: 0.903\n",
      "Epoch: 23.000,  Val Accuracy: 0.868\n",
      "Epoch: 23.000, Test Accuracy: 0.869\n",
      "Test Accuracy: 0.8689422671723557 \n",
      "Test Accuracy over subgroups: [0.78378378 0.7523511  0.69886364 0.73913043 0.96055227 0.95794393\n",
      " 0.82296651 0.828125  ] \n",
      "Test Worst Group Accuracy: 0.6988636363636364\n",
      "Epoch: 24.000, Loss: 0.234, Train_Accuracy: 0.904\n",
      "Epoch: 24.000,  Val Accuracy: 0.864\n",
      "Epoch: 24.000, Test Accuracy: 0.868\n",
      "Test Accuracy: 0.8680994521702486 \n",
      "Test Accuracy over subgroups: [0.85585586 0.7460815  0.66477273 0.73913043 0.96055227 0.94392523\n",
      " 0.81818182 0.83854167] \n",
      "Test Worst Group Accuracy: 0.6647727272727273\n",
      "Epoch: 25.000, Loss: 0.230, Train_Accuracy: 0.906\n",
      "Epoch: 25.000,  Val Accuracy: 0.861\n",
      "Epoch: 25.000, Test Accuracy: 0.858\n",
      "Test Accuracy: 0.8575642646439107 \n",
      "Test Accuracy over subgroups: [0.86486486 0.80564263 0.76136364 0.76811594 0.93688363 0.87850467\n",
      " 0.72727273 0.79166667] \n",
      "Test Worst Group Accuracy: 0.7272727272727273\n",
      "Epoch: 26.000, Loss: 0.227, Train_Accuracy: 0.908\n",
      "Epoch: 26.000,  Val Accuracy: 0.857\n",
      "Epoch: 26.000, Test Accuracy: 0.857\n",
      "Test Accuracy: 0.8571428571428571 \n",
      "Test Accuracy over subgroups: [0.83783784 0.73981191 0.69318182 0.73188406 0.94773176 0.93925234\n",
      " 0.77990431 0.81770833] \n",
      "Test Worst Group Accuracy: 0.6931818181818182\n",
      "Epoch: 27.000, Loss: 0.224, Train_Accuracy: 0.911\n",
      "Epoch: 27.000,  Val Accuracy: 0.858\n",
      "Epoch: 27.000, Test Accuracy: 0.858\n",
      "Test Accuracy: 0.8584070796460177 \n",
      "Test Accuracy over subgroups: [0.81981982 0.72413793 0.66477273 0.73913043 0.9556213  0.92990654\n",
      " 0.79904306 0.83854167] \n",
      "Test Worst Group Accuracy: 0.6647727272727273\n",
      "Epoch: 28.000, Loss: 0.220, Train_Accuracy: 0.911\n",
      "Epoch: 28.000,  Val Accuracy: 0.845\n",
      "Epoch: 28.000, Test Accuracy: 0.847\n",
      "Test Accuracy: 0.8470290771175727 \n",
      "Test Accuracy over subgroups: [0.79279279 0.70846395 0.65909091 0.70289855 0.95069034 0.92990654\n",
      " 0.77511962 0.82291667] \n",
      "Test Worst Group Accuracy: 0.6590909090909091\n",
      "Epoch: 29.000, Loss: 0.217, Train_Accuracy: 0.911\n",
      "Epoch: 29.000,  Val Accuracy: 0.845\n",
      "Epoch: 29.000, Test Accuracy: 0.850\n",
      "Test Accuracy: 0.8499789296249474 \n",
      "Test Accuracy over subgroups: [0.83783784 0.69592476 0.60795455 0.72463768 0.9566075  0.92990654\n",
      " 0.784689   0.84375   ] \n",
      "Test Worst Group Accuracy: 0.6079545454545454\n",
      "Epoch: 30.000, Loss: 0.214, Train_Accuracy: 0.914\n",
      "Epoch: 30.000,  Val Accuracy: 0.838\n",
      "Epoch: 30.000, Test Accuracy: 0.838\n",
      "Test Accuracy: 0.8381795195954488 \n",
      "Test Accuracy over subgroups: [0.81981982 0.65517241 0.57386364 0.69565217 0.94378698 0.93457944\n",
      " 0.8277512  0.84375   ] \n",
      "Test Worst Group Accuracy: 0.5738636363636364\n",
      "Epoch: 31.000, Loss: 0.211, Train_Accuracy: 0.914\n",
      "Epoch: 31.000,  Val Accuracy: 0.841\n",
      "Epoch: 31.000, Test Accuracy: 0.832\n",
      "Test Accuracy: 0.831858407079646 \n",
      "Test Accuracy over subgroups: [0.84684685 0.68025078 0.55681818 0.72463768 0.93491124 0.92990654\n",
      " 0.77033493 0.81770833] \n",
      "Test Worst Group Accuracy: 0.5568181818181818\n",
      "Epoch: 32.000, Loss: 0.208, Train_Accuracy: 0.914\n",
      "Epoch: 32.000,  Val Accuracy: 0.853\n",
      "Epoch: 32.000, Test Accuracy: 0.850\n",
      "Test Accuracy: 0.8504003371260008 \n",
      "Test Accuracy over subgroups: [0.82882883 0.70219436 0.60227273 0.7173913  0.95463511 0.92990654\n",
      " 0.79425837 0.85416667] \n",
      "Test Worst Group Accuracy: 0.6022727272727273\n",
      "Epoch: 33.000, Loss: 0.206, Train_Accuracy: 0.915\n",
      "Epoch: 33.000,  Val Accuracy: 0.842\n",
      "Epoch: 33.000, Test Accuracy: 0.839\n",
      "Test Accuracy: 0.8390223345975558 \n",
      "Test Accuracy over subgroups: [0.82882883 0.68338558 0.57386364 0.7173913  0.94477318 0.91588785\n",
      " 0.78947368 0.84375   ] \n",
      "Test Worst Group Accuracy: 0.5738636363636364\n",
      "Epoch: 34.000, Loss: 0.203, Train_Accuracy: 0.918\n",
      "Epoch: 34.000,  Val Accuracy: 0.841\n",
      "Epoch: 34.000, Test Accuracy: 0.840\n",
      "Test Accuracy: 0.8398651495996629 \n",
      "Test Accuracy over subgroups: [0.85585586 0.69278997 0.54545455 0.7173913  0.94871795 0.91121495\n",
      " 0.784689   0.83854167] \n",
      "Test Worst Group Accuracy: 0.5454545454545454\n",
      "Epoch: 35.000, Loss: 0.201, Train_Accuracy: 0.921\n",
      "Epoch: 35.000,  Val Accuracy: 0.833\n",
      "Epoch: 35.000, Test Accuracy: 0.837\n",
      "Test Accuracy: 0.8369152970922883 \n",
      "Test Accuracy over subgroups: [0.81981982 0.67711599 0.52272727 0.71014493 0.94871795 0.91121495\n",
      " 0.79425837 0.86458333] \n",
      "Test Worst Group Accuracy: 0.5227272727272727\n",
      "Epoch: 36.000, Loss: 0.199, Train_Accuracy: 0.922\n",
      "Epoch: 36.000,  Val Accuracy: 0.813\n",
      "Epoch: 36.000, Test Accuracy: 0.824\n",
      "Test Accuracy: 0.8238516645596292 \n",
      "Test Accuracy over subgroups: [0.78378378 0.64263323 0.49431818 0.69565217 0.9408284  0.90186916\n",
      " 0.81339713 0.84895833] \n",
      "Test Worst Group Accuracy: 0.4943181818181818\n",
      "Epoch: 37.000, Loss: 0.196, Train_Accuracy: 0.922\n",
      "Epoch: 37.000,  Val Accuracy: 0.834\n",
      "Epoch: 37.000, Test Accuracy: 0.831\n",
      "Test Accuracy: 0.8305941845764855 \n",
      "Test Accuracy over subgroups: [0.81081081 0.68965517 0.53409091 0.70289855 0.93786982 0.90186916\n",
      " 0.79425837 0.83333333] \n",
      "Test Worst Group Accuracy: 0.5340909090909091\n",
      "Epoch: 38.000, Loss: 0.194, Train_Accuracy: 0.922\n",
      "Epoch: 38.000,  Val Accuracy: 0.816\n",
      "Epoch: 38.000, Test Accuracy: 0.827\n",
      "Test Accuracy: 0.8268015170670038 \n",
      "Test Accuracy over subgroups: [0.8018018  0.6645768  0.5        0.70289855 0.93688363 0.90186916\n",
      " 0.81818182 0.84375   ] \n",
      "Test Worst Group Accuracy: 0.5\n",
      "Epoch: 39.000, Loss: 0.191, Train_Accuracy: 0.923\n",
      "Epoch: 39.000,  Val Accuracy: 0.828\n",
      "Epoch: 39.000, Test Accuracy: 0.825\n",
      "Test Accuracy: 0.8246944795617362 \n",
      "Test Accuracy over subgroups: [0.87387387 0.69905956 0.51136364 0.72463768 0.92899408 0.88317757\n",
      " 0.75598086 0.82291667] \n",
      "Test Worst Group Accuracy: 0.5113636363636364\n",
      "Epoch: 40.000, Loss: 0.189, Train_Accuracy: 0.923\n",
      "Epoch: 40.000,  Val Accuracy: 0.826\n",
      "Epoch: 40.000, Test Accuracy: 0.834\n",
      "Test Accuracy: 0.83354403708386 \n",
      "Test Accuracy over subgroups: [0.82882883 0.65830721 0.50568182 0.72463768 0.93984221 0.91121495\n",
      " 0.83732057 0.85416667] \n",
      "Test Worst Group Accuracy: 0.5056818181818182\n",
      "Epoch: 41.000, Loss: 0.188, Train_Accuracy: 0.926\n",
      "Epoch: 41.000,  Val Accuracy: 0.832\n",
      "Epoch: 41.000, Test Accuracy: 0.835\n",
      "Test Accuracy: 0.8348082595870207 \n",
      "Test Accuracy over subgroups: [0.86486486 0.66144201 0.49431818 0.70289855 0.94378698 0.90654206\n",
      " 0.80861244 0.88541667] \n",
      "Test Worst Group Accuracy: 0.4943181818181818\n",
      "Epoch: 42.000, Loss: 0.186, Train_Accuracy: 0.926\n",
      "Epoch: 42.000,  Val Accuracy: 0.828\n",
      "Epoch: 42.000, Test Accuracy: 0.831\n",
      "Test Accuracy: 0.831015592077539 \n",
      "Test Accuracy over subgroups: [0.8018018  0.6645768  0.48863636 0.6884058  0.94871795 0.90654206\n",
      " 0.80861244 0.859375  ] \n",
      "Test Worst Group Accuracy: 0.48863636363636365\n",
      "Epoch: 43.000, Loss: 0.183, Train_Accuracy: 0.927\n",
      "Epoch: 43.000,  Val Accuracy: 0.817\n",
      "Epoch: 43.000, Test Accuracy: 0.832\n",
      "Test Accuracy: 0.8322798145806996 \n",
      "Test Accuracy over subgroups: [0.76576577 0.60815047 0.44886364 0.68115942 0.95463511 0.91588785\n",
      " 0.87559809 0.91666667] \n",
      "Test Worst Group Accuracy: 0.44886363636363635\n",
      "Epoch: 44.000, Loss: 0.181, Train_Accuracy: 0.928\n",
      "Epoch: 44.000,  Val Accuracy: 0.821\n",
      "Epoch: 44.000, Test Accuracy: 0.825\n",
      "Test Accuracy: 0.8251158870627897 \n",
      "Test Accuracy over subgroups: [0.71171171 0.5862069  0.40909091 0.61594203 0.96449704 0.92990654\n",
      " 0.89473684 0.890625  ] \n",
      "Test Worst Group Accuracy: 0.4090909090909091\n",
      "Epoch: 45.000, Loss: 0.180, Train_Accuracy: 0.931\n",
      "Epoch: 45.000,  Val Accuracy: 0.830\n",
      "Epoch: 45.000, Test Accuracy: 0.836\n",
      "Test Accuracy: 0.8360724820901813 \n",
      "Test Accuracy over subgroups: [0.75675676 0.63322884 0.46590909 0.64492754 0.96153846 0.94392523\n",
      " 0.8708134  0.875     ] \n",
      "Test Worst Group Accuracy: 0.4659090909090909\n",
      "Epoch: 46.000, Loss: 0.177, Train_Accuracy: 0.931\n",
      "Epoch: 46.000,  Val Accuracy: 0.816\n",
      "Epoch: 46.000, Test Accuracy: 0.819\n",
      "Test Accuracy: 0.8192161820480405 \n",
      "Test Accuracy over subgroups: [0.69369369 0.5830721  0.40909091 0.66666667 0.95069034 0.94392523\n",
      " 0.88516746 0.86458333] \n",
      "Test Worst Group Accuracy: 0.4090909090909091\n",
      "Epoch: 47.000, Loss: 0.175, Train_Accuracy: 0.932\n",
      "Epoch: 47.000,  Val Accuracy: 0.809\n",
      "Epoch: 47.000, Test Accuracy: 0.823\n",
      "Test Accuracy: 0.8225874420564686 \n",
      "Test Accuracy over subgroups: [0.72972973 0.61442006 0.44318182 0.64492754 0.94970414 0.92990654\n",
      " 0.86602871 0.859375  ] \n",
      "Test Worst Group Accuracy: 0.4431818181818182\n",
      "Epoch: 48.000, Loss: 0.173, Train_Accuracy: 0.934\n",
      "Epoch: 48.000,  Val Accuracy: 0.813\n",
      "Epoch: 48.000, Test Accuracy: 0.820\n",
      "Test Accuracy: 0.820480404551201 \n",
      "Test Accuracy over subgroups: [0.69369369 0.57680251 0.42045455 0.60869565 0.95857988 0.95327103\n",
      " 0.88995215 0.86458333] \n",
      "Test Worst Group Accuracy: 0.42045454545454547\n",
      "Epoch: 49.000, Loss: 0.171, Train_Accuracy: 0.934\n",
      "Epoch: 49.000,  Val Accuracy: 0.806\n",
      "Epoch: 49.000, Test Accuracy: 0.817\n",
      "Test Accuracy: 0.8171091445427728 \n",
      "Test Accuracy over subgroups: [0.7027027  0.57680251 0.41477273 0.62318841 0.95167653 0.94392523\n",
      " 0.89473684 0.85416667] \n",
      "Test Worst Group Accuracy: 0.4147727272727273\n",
      "Epoch: 50.000, Loss: 0.169, Train_Accuracy: 0.934\n",
      "Epoch: 50.000,  Val Accuracy: 0.806\n",
      "Epoch: 50.000, Test Accuracy: 0.817\n",
      "Test Accuracy: 0.8171091445427728 \n",
      "Test Accuracy over subgroups: [0.7027027  0.55485893 0.39204545 0.63043478 0.9566075  0.94392523\n",
      " 0.89952153 0.875     ] \n",
      "Test Worst Group Accuracy: 0.39204545454545453\n",
      "Epoch: 51.000, Loss: 0.167, Train_Accuracy: 0.936\n",
      "Epoch: 51.000,  Val Accuracy: 0.802\n",
      "Epoch: 51.000, Test Accuracy: 0.814\n",
      "Test Accuracy: 0.8141592920353983 \n",
      "Test Accuracy over subgroups: [0.69369369 0.56739812 0.39204545 0.5942029  0.95463511 0.94859813\n",
      " 0.88995215 0.86458333] \n",
      "Test Worst Group Accuracy: 0.39204545454545453\n",
      "Epoch: 52.000, Loss: 0.166, Train_Accuracy: 0.936\n",
      "Epoch: 52.000,  Val Accuracy: 0.809\n",
      "Epoch: 52.000, Test Accuracy: 0.822\n",
      "Test Accuracy: 0.822166034555415 \n",
      "Test Accuracy over subgroups: [0.73873874 0.58934169 0.40909091 0.60869565 0.9566075  0.94392523\n",
      " 0.88995215 0.86979167] \n",
      "Test Worst Group Accuracy: 0.4090909090909091\n",
      "Epoch: 53.000, Loss: 0.164, Train_Accuracy: 0.935\n",
      "Epoch: 53.000,  Val Accuracy: 0.791\n",
      "Epoch: 53.000, Test Accuracy: 0.802\n",
      "Test Accuracy: 0.8023598820058997 \n",
      "Test Accuracy over subgroups: [0.63063063 0.50470219 0.36931818 0.51449275 0.96153846 0.95327103\n",
      " 0.90909091 0.875     ] \n",
      "Test Worst Group Accuracy: 0.3693181818181818\n",
      "Epoch: 54.000, Loss: 0.162, Train_Accuracy: 0.938\n",
      "Epoch: 54.000,  Val Accuracy: 0.819\n",
      "Epoch: 54.000, Test Accuracy: 0.828\n",
      "Test Accuracy: 0.8276443320691108 \n",
      "Test Accuracy over subgroups: [0.66666667 0.55799373 0.39204545 0.61594203 0.97337278 0.95794393\n",
      " 0.91866029 0.90625   ] \n",
      "Test Worst Group Accuracy: 0.39204545454545453\n",
      "Epoch: 55.000, Loss: 0.161, Train_Accuracy: 0.938\n",
      "Epoch: 55.000,  Val Accuracy: 0.814\n",
      "Epoch: 55.000, Test Accuracy: 0.824\n",
      "Test Accuracy: 0.8238516645596292 \n",
      "Test Accuracy over subgroups: [0.68468468 0.56739812 0.39204545 0.60869565 0.96844181 0.94859813\n",
      " 0.89952153 0.89583333] \n",
      "Test Worst Group Accuracy: 0.39204545454545453\n",
      "Epoch: 56.000, Loss: 0.159, Train_Accuracy: 0.939\n",
      "Epoch: 56.000,  Val Accuracy: 0.813\n",
      "Epoch: 56.000, Test Accuracy: 0.818\n",
      "Test Accuracy: 0.8183733670459334 \n",
      "Test Accuracy over subgroups: [0.63963964 0.57366771 0.39772727 0.52173913 0.97140039 0.94392523\n",
      " 0.90430622 0.88541667] \n",
      "Test Worst Group Accuracy: 0.3977272727272727\n",
      "Epoch: 57.000, Loss: 0.157, Train_Accuracy: 0.941\n",
      "Epoch: 57.000,  Val Accuracy: 0.821\n",
      "Epoch: 57.000, Test Accuracy: 0.830\n",
      "Test Accuracy: 0.8301727770754319 \n",
      "Test Accuracy over subgroups: [0.69369369 0.5799373  0.42045455 0.62318841 0.97140039 0.94859813\n",
      " 0.90909091 0.88541667] \n",
      "Test Worst Group Accuracy: 0.42045454545454547\n",
      "Epoch: 58.000, Loss: 0.156, Train_Accuracy: 0.942\n",
      "Epoch: 58.000,  Val Accuracy: 0.827\n",
      "Epoch: 58.000, Test Accuracy: 0.837\n",
      "Test Accuracy: 0.8369152970922883 \n",
      "Test Accuracy over subgroups: [0.73873874 0.63322884 0.45454545 0.65217391 0.96449704 0.93925234\n",
      " 0.88038278 0.88020833] \n",
      "Test Worst Group Accuracy: 0.45454545454545453\n",
      "For Trial: 0 Test Accuracy: 0.8369152970922883 \n",
      "Test Accuracy over each subgroups: [0.73873874 0.63322884 0.45454545 0.65217391 0.96449704 0.93925234\n",
      " 0.88038278 0.88020833] \n",
      "test Worst Group Accuracy: 0.45454545454545453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1 = df1.append({'trial': i, 'subtype': 'overall', 'Train gDRO accuracy': temptrain_acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:44: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append({'trial': i, 'subtype': 'overall', 'Val gDRO accuracy': tempval_acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_12276\\1757810102.py:50: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df3 = df3.append({'trial': i, 'subtype': 'overall', 'Test gDRO accuracy': temptest_acc}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# number of epochs\n",
    "n_epochs = 58\n",
    "tr_loss = []\n",
    "tr_accuracies = []\n",
    "val_loss = []\n",
    "val_accuracies = []\n",
    "test_loss = []\n",
    "test_accuracies = []\n",
    "max_worst_accuracy = 0\n",
    "\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "num_trials = 1\n",
    "df1 = pd.DataFrame(columns=['trial', 'subtype', 'Train gDRO accuracy'])\n",
    "df2 = pd.DataFrame(columns=['trial', 'subtype', 'Val gDRO accuracy'])\n",
    "df3 = pd.DataFrame(columns=['trial', 'subtype', 'Test gDRO accuracy'])\n",
    "subgroups = {'adenosis',\n",
    "            'fibroadenoma',\n",
    "            'tubular_adenoma',\n",
    "            'phyllodes_tumor',\n",
    "            'ductal_carcinoma',\n",
    "            'lobular_carcinoma',\n",
    "            'mucinous_carcinoma',\n",
    "            'papillary_carcinoma'}\n",
    "\n",
    "# training the model\n",
    "for i in range(num_trials):\n",
    "    for epoch in range(n_epochs):\n",
    "        temptrain_loss, temptrain_acc,trainsubgroup_acc= train(epoch)\n",
    "        tempval_acc, valsubgroup_acc = val(epoch)\n",
    "        temptest_acc, testsubgroup_acc = test(epoch)\n",
    "        \n",
    "    # append accuracy values for each subgroup to the dataframes\n",
    "    for j, acc in zip(subgroups, trainsubgroup_acc):\n",
    "        df1 = df1.append({'trial': i, 'subtype': j, 'Train gDRO accuracy': acc}, ignore_index=True)\n",
    "    \n",
    "    # add overall accuracy to Train gDRO accuracy for each trial\n",
    "    df1 = df1.append({'trial': i, 'subtype': 'overall', 'Train gDRO accuracy': temptrain_acc}, ignore_index=True)\n",
    "    \n",
    "    for j, acc in zip(subgroups, valsubgroup_acc):\n",
    "        df2 = df2.append({'trial': i, 'subtype': j, 'Val gDRO accuracy': acc}, ignore_index=True)\n",
    "    \n",
    "    # add overall accuracy to Val gDRO accuracy for each trial\n",
    "    df2 = df2.append({'trial': i, 'subtype': 'overall', 'Val gDRO accuracy': tempval_acc}, ignore_index=True)\n",
    "    \n",
    "    for j, acc in zip(subgroups, testsubgroup_acc):\n",
    "        df3 = df3.append({'trial': i, 'subtype': j, 'Test gDRO accuracy': acc}, ignore_index=True)\n",
    "    \n",
    "    # add overall accuracy to Test gDRO accuracy for each trial\n",
    "    df3 = df3.append({'trial': i, 'subtype': 'overall', 'Test gDRO accuracy': temptest_acc}, ignore_index=True)\n",
    "\n",
    "        # Check if the validation loss has increased and update the best model if it hasn't\n",
    "        # a =  min(valsubgroup_acc)\n",
    "        # print(a)\n",
    "        # if a >= max_worst_accuracy:\n",
    "        #     max_worst_accuracy = a\n",
    "        #     best_epoch = epoch\n",
    "        #     print('I am saving the best dro model at Epoch: ', best_epoch)\n",
    "        #     torch.save(model.state_dict(), r'Best_gdromodel.pth')\n",
    "        #     counter = 0\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     if counter >= patience: # Stop early if the validation loss has increased for `patience` epochs\n",
    "        #         break\n",
    "     \n",
    "    # print(\"For Trial:\",i,\"Train Accuracy:\", temptrain_acc, \"\\nTrain Accuracy over each subgroups:\", trainsubgroup_acc, \"\\nTrain Worst Group Accuracy:\",min(trainsubgroup_acc))\n",
    "    # print(\"For Trial:\",i,\"Val Accuracy:\", tempval_acc, \"\\nVal Accuracy over each subgroups:\", valsubgroup_acc, \"\\nVal Worst Group Accuracy:\",min(valsubgroup_acc))\n",
    "    print(\"For Trial:\",i,\"Test Accuracy:\", temptest_acc,\"\\nTest Accuracy over each subgroups:\", testsubgroup_acc, \"\\ntest Worst Group Accuracy:\",min(testsubgroup_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that df1 is your data frame\n",
    "df1.to_csv('Train_gdroccuracies.csv', index=False)\n",
    "df2.to_csv('Val_gdroccuracies.csv', index=False)\n",
    "df3.to_csv('Test_gdroaccuracies.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
