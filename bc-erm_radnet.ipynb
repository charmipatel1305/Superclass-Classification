{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import Linear, ReLU, BCEWithLogitsLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, Dropout\n",
    "from torch.optim import Adam,SGD\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Subset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA Version is 11.3\n",
      "Whether CUDA is supported by our system: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch CUDA Version is\", torch.version.cuda)\n",
    "print(\"Whether CUDA is supported by our system:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.data.to_string(index=False)\n",
    "#         self.transform = transform\n",
    "#         self.data['Super Class'] = self.data['Super Class'].apply(lambda x: 0 if x == 'benign' else 1)\n",
    "#         label_map = {\n",
    "#             'adenosis': 0,\n",
    "#             'fibroadenoma': 1,\n",
    "#             'tubular_adenoma': 2,\n",
    "#             'phyllodes_tumor': 3,\n",
    "#             'ductal_carcinoma': 4,\n",
    "#             'lobular_carcinoma': 5,\n",
    "#             'mucinous_carcinoma': 6,\n",
    "#             'papillary_carcinoma': 7\n",
    "#         }\n",
    "#         self.data['Sub Class'] = self.data['Sub Class'].map(label_map)\n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_path = self.data.iloc[idx, 0]\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         SuperClass = self.data.iloc[idx, 1]\n",
    "#         # SuperClass = torch.tensor(SuperClass, dtype=torch.long)\n",
    "#         SubClass = self.data.iloc[idx, 3]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         image = asarray(image)\n",
    "#         return (image, SuperClass, SubClass)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256,256)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# dataset = ImageDataset(csv_file='csv windows.csv', transform=transform)\n",
    "# images = [x[0] for x in dataset]\n",
    "# superclass_labels = [x[1] for x in dataset]\n",
    "# subclass_labels = [x[2] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Serialize and save the list to a file\n",
    "# with open('image_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(images, f)\n",
    "\n",
    "# with open('superclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(superclass_labels, f)\n",
    "    \n",
    "\n",
    "# with open('subclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(subclass_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_list.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\One Drive\\OneDrive - DePaul University\\Datasets\\Github\\Superclass-Classification\\bc-erm_radnet.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/Github/Superclass-Classification/bc-erm_radnet.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# file_pathimages = '~/superclass/data/images/images/image_list.pkl'\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/Github/Superclass-Classification/bc-erm_radnet.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mimage_list.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/Github/Superclass-Classification/bc-erm_radnet.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     images \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/Github/Superclass-Classification/bc-erm_radnet.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msuperclass_labels.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_list.pkl'"
     ]
    }
   ],
   "source": [
    "# file_pathimages = '~/superclass/data/images/images/image_list.pkl'\n",
    "with open('image_list.pkl', 'rb') as f:\n",
    "    images = pickle.load(f)\n",
    "\n",
    "with open('superclass_labels.pkl', 'rb') as f:\n",
    "    superclass_labels = pickle.load(f)\n",
    "\n",
    "with open('subclass_labels.pkl', 'rb') as f:\n",
    "    subclass_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(images, superclass_labels, subclass_labels, test_size=0.3, stratify=superclass_labels, random_state=42)\n",
    "x_train, x_val, y_train, y_val, z_train, z_val = train_test_split(x_train, y_train, z_train, test_size=0.3, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Train=226, Val=107, Test=111\n",
      "Class 1: Train=484, Val=211, Test=319\n",
      "Class 2: Train=282, Val=111, Test=176\n",
      "Class 3: Train=223, Val=92, Test=138\n",
      "Class 4: Train=1700, Val=737, Test=1014\n",
      "Class 5: Train=284, Val=128, Test=214\n",
      "Class 6: Train=408, Val=175, Test=209\n",
      "Class 7: Train=268, Val=100, Test=192\n"
     ]
    }
   ],
   "source": [
    "# get the unique labels in y\n",
    "unique_labels = np.unique(subclass_labels)\n",
    "\n",
    "# count the number of samples in each set for each class\n",
    "train_counts = [np.sum(z_train == label) for label in unique_labels]\n",
    "val_counts = [np.sum(z_val == label) for label in unique_labels]\n",
    "test_counts = [np.sum(z_test == label) for label in unique_labels]\n",
    "\n",
    "# print the counts for each class in each set\n",
    "for i, label in enumerate(unique_labels): \n",
    "    print(f\"Class {label}: Train={train_counts[i]}, Val={val_counts[i]}, Test={test_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.z[index]\n",
    "\n",
    "train_dataset = YourDataset(x_train, y_train, z_train)\n",
    "val_dataset = YourDataset(x_val, y_val, z_val)\n",
    "test_dataset = YourDataset(x_test, y_test, z_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('RadImageNet-ResNet50_notop.h5')\n",
    "keras_weights = model.get_weights()\n",
    "# print(keras_weights)\n",
    "\n",
    "layer_names = []\n",
    "\n",
    "for i, layer in enumerate(model.layers):\n",
    "    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.BatchNormalization)):\n",
    "          layer_names.append(layer.name)\n",
    "\n",
    "        #   print(layer.name)\n",
    "# print(layer_names)\n",
    "\n",
    "# Loop through the layers and create a dictionary mapping layer names to their associated weights\n",
    "weights_dict = {layer.name: layer.get_weights() for layer in model.layers if isinstance(layer, tf.keras.layers.Layer) and layer.weights}\n",
    "\n",
    "# Print the weights_dict to check its contents\n",
    "# print(weights_dict)\n",
    "\n",
    "\n",
    "new_weights = []\n",
    "new_layer_order = ['conv1_conv', 'conv1_bn', 'conv2_block1_1_conv', 'conv2_block1_1_bn', 'conv2_block1_2_conv',\n",
    "'conv2_block1_2_bn','conv2_block1_3_conv','conv2_block1_3_bn','conv2_block1_0_conv','conv2_block1_0_bn',\n",
    "'conv2_block2_1_conv',\n",
    "'conv2_block2_1_bn',\n",
    "'conv2_block2_2_conv',\n",
    "'conv2_block2_2_bn',\n",
    "'conv2_block2_3_conv',\n",
    "'conv2_block2_3_bn',\n",
    "'conv2_block3_1_conv',\n",
    "'conv2_block3_1_bn',\n",
    "'conv2_block3_2_conv',\n",
    "'conv2_block3_2_bn',\n",
    "'conv2_block3_3_conv',\n",
    "'conv2_block3_3_bn',\n",
    "\n",
    "'conv3_block1_1_conv',\n",
    "'conv3_block1_1_bn',\n",
    "'conv3_block1_2_conv',\n",
    "'conv3_block1_2_bn',\n",
    "'conv3_block1_3_conv',\n",
    "'conv3_block1_3_bn',\n",
    "'conv3_block1_0_conv',\n",
    "'conv3_block1_0_bn',\n",
    "'conv3_block2_1_conv',\n",
    "'conv3_block2_1_bn',\n",
    "'conv3_block2_2_conv',\n",
    "'conv3_block2_2_bn',\n",
    "'conv3_block2_3_conv',\n",
    "'conv3_block2_3_bn',\n",
    "'conv3_block3_1_conv',\n",
    "'conv3_block3_1_bn',\n",
    "'conv3_block3_2_conv',\n",
    "'conv3_block3_2_bn',\n",
    "'conv3_block3_3_conv',\n",
    "'conv3_block3_3_bn',\n",
    "'conv3_block4_1_conv',\n",
    "'conv3_block4_1_bn',\n",
    "'conv3_block4_2_conv',\n",
    "'conv3_block4_2_bn',\n",
    "'conv3_block4_3_conv',\n",
    "'conv3_block4_3_bn',\n",
    "\n",
    "'conv4_block1_1_conv',\n",
    "'conv4_block1_1_bn',\n",
    "'conv4_block1_2_conv',\n",
    "'conv4_block1_2_bn',\n",
    "'conv4_block1_3_conv',\n",
    "'conv4_block1_3_bn',\n",
    "'conv4_block1_0_conv',\n",
    "'conv4_block1_0_bn',\n",
    "'conv4_block2_1_conv',\n",
    "'conv4_block2_1_bn',\n",
    "'conv4_block2_2_conv',\n",
    "'conv4_block2_2_bn',\n",
    "'conv4_block2_3_conv',\n",
    "'conv4_block2_3_bn',\n",
    "'conv4_block3_1_conv',\n",
    "'conv4_block3_1_bn',\n",
    "'conv4_block3_2_conv',\n",
    "'conv4_block3_2_bn',\n",
    "'conv4_block3_3_conv',\n",
    "'conv4_block3_3_bn',\n",
    "'conv4_block4_1_conv',\n",
    "'conv4_block4_1_bn',\n",
    "'conv4_block4_2_conv',\n",
    "'conv4_block4_2_bn',\n",
    "'conv4_block4_3_conv',\n",
    "'conv4_block4_3_bn',\n",
    "'conv4_block5_1_conv',\n",
    "'conv4_block5_1_bn',\n",
    "'conv4_block5_2_conv',\n",
    "'conv4_block5_2_bn',\n",
    "'conv4_block5_3_conv',\n",
    "'conv4_block5_3_bn',\n",
    "'conv4_block6_1_conv',\n",
    "'conv4_block6_1_bn',\n",
    "'conv4_block6_2_conv',\n",
    "'conv4_block6_2_bn',\n",
    "'conv4_block6_3_conv',\n",
    "'conv4_block6_3_bn',\n",
    "\n",
    "'conv5_block1_1_conv',\n",
    "'conv5_block1_1_bn',\n",
    "'conv5_block1_2_conv',\n",
    "'conv5_block1_2_bn',\n",
    "'conv5_block1_3_conv',\n",
    "'conv5_block1_3_bn',\n",
    "'conv5_block1_0_conv',\n",
    "'conv5_block1_0_bn',\n",
    "'conv5_block2_1_conv',\n",
    "'conv5_block2_1_bn',\n",
    "'conv5_block2_2_conv',\n",
    "'conv5_block2_2_bn',\n",
    "'conv5_block2_3_conv',\n",
    "'conv5_block2_3_bn',\n",
    "'conv5_block3_1_conv',\n",
    "'conv5_block3_1_bn',\n",
    "'conv5_block3_2_conv',\n",
    "'conv5_block3_2_bn',\n",
    "'conv5_block3_3_conv', 'conv5_block3_3_bn']\n",
    "\n",
    "# Rearrange the weights according to the new layer order\n",
    "for layer_name in new_layer_order:\n",
    "    weight = weights_dict[layer_name]\n",
    "    # print(weight)\n",
    "    new_weights.append(weight)\n",
    "\n",
    "# print(new_weights[1][0])\n",
    "#remove bias of convolutional layer\n",
    "new_weights_wo_convbias = []\n",
    "\n",
    "for i, weight in enumerate(new_weights):\n",
    "    if i % 6 == 1:\n",
    "        continue  # Skip the indices to neglect\n",
    "    new_weights_wo_convbias.append(weight)\n",
    "\n",
    "# print(new_weights_wo_convbias)\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the weights\n",
    "keras_weights = new_weights\n",
    "# print(keras_weights)\n",
    "# Load the PyTorch model\n",
    "resnet_model = torchvision.models.resnet50(weights=True) #implemented based on the previous model (by myself)\n",
    "new_state_dict = {}\n",
    "with torch.no_grad():\n",
    "    y = 1\n",
    "    for i, layer in enumerate(resnet_model.modules()):\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            # extract the weights and biases from the TensorFlow weights\n",
    "            weight_tf_conv = keras_weights[(2*y)-2][0]\n",
    "            # print(weight_tf_conv)\n",
    "            \n",
    "            layer.weight.data = torch.tensor(weight_tf_conv.transpose(), dtype=torch.float)\n",
    "            # print(layer.weight.data)\n",
    "            # print(layer)\n",
    "\n",
    "        if isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            weights_tf_batch = keras_weights[(2*y)-1][0]\n",
    "            bias_tf_batch = keras_weights[(2*y)-1][1]\n",
    "            layer.weight.data= torch.tensor(weights_tf_batch, dtype=torch.float)\n",
    "            layer.bias.data = torch.tensor(bias_tf_batch, dtype=torch.float)\n",
    "            y = y+1\n",
    "\n",
    "\n",
    "resnet_model.load_state_dict(new_state_dict, strict=False)\n",
    "# Print the PyTorch model layer name and its pre-trained weights\n",
    "# for name, param in resnet_model.named_parameters():\n",
    "#     print(name)\n",
    "#     print(param)\n",
    "\n",
    "lt=10\n",
    "cntr = 0\n",
    "for child in resnet_model.children():\n",
    "    cntr+=1\n",
    "\n",
    "    if cntr < lt:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "resnet_model.fc = torch.nn.Linear(in_features = num_ftrs, out_features = 1, bias=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "\n",
    "# optimizer and loss function\n",
    "\n",
    "# define cnn model\n",
    "model = resnet_model.to(device)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# define loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train(epoch):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tr_loss=0\n",
    "    model.train()\n",
    "    \n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, subclass = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        # print(labels)\n",
    "        # print(subclass)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # predcition for training set\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        \n",
    "        outputs = outputs.flatten()\n",
    "        total += labels.size(0)\n",
    "        # print(total)\n",
    "        \n",
    "        y_2 = torch.zeros(len(outputs))\n",
    "        y_2[outputs>=0.0] = 1\n",
    "        y_2= y_2.int()\n",
    "        y_2 = y_2.to(device)\n",
    "        \n",
    "        for subclasses in range(num_subclasses):\n",
    "            subclass_idx = subclass == subclasses\n",
    "            num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "            subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                torch.float).sum().item()\n",
    "\n",
    "        subgroup_accuracy = subgroup_correct / num_samples\n",
    "        # print(y_2)\n",
    "        correct += (y_2 == labels).sum().item()\n",
    "        train_accuracy = correct / total \n",
    "        # for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "        #         confusion_matrix[t.long(), p.long()] += 1\n",
    "        \n",
    "        # clearing the gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute loss\n",
    "        loss_train = criterion(outputs, labels)\n",
    "        # print(loss_train)\n",
    "        \n",
    "        # compute updates weights of all the parameters\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss_train.item()\n",
    "\n",
    "    # Calculate training loss value\n",
    "    train_loss_value = tr_loss/len(train_loader) \n",
    "    print(\"Epoch: {:.3f}, Loss: {:.3f}, Train_Accuracy: {:.3f}\".format(epoch+1, train_loss_value, train_accuracy)) \n",
    "    # print('confusion matrix of training images: {}'.format(confusion_matrix))\n",
    "\n",
    "    # print(\"Train Accuracy:\", accuracy, \"\\nTrain Accuracy over subgroups:\", subgroup_accuracy, \"\\nTrain Worst Group Accuracy:\",\n",
    "    #           min(subgroup_accuracy))\n",
    "    \n",
    "    return train_accuracy, subgroup_accuracy \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            # print(labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            loss_train = criterion(outputs, labels)\n",
    "            tr_loss += loss_train.item()\n",
    "            val_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            \n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            \n",
    "        print(\"Epoch: {:.3f}, Loss: {:.3f}, Val Accuracy: {:.3f}\".format(epoch+1, val_loss_value, val_accuracy)) \n",
    "        # print('confusion matrix of validation images: {}'.format(confusion_matrix)) \n",
    "        # print(\"Val Accuracy:\", accuracy, \"\\nVal Accuracy over subgroups:\", subgroup_accuracy, \"\\nVal Worst Group Accuracy:\",\n",
    "        #       min(subgroup_accuracy))       \n",
    "    \n",
    "        return val_accuracy, subgroup_accuracy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        num_subclasses = 8\n",
    "        num_samples = np.zeros(num_subclasses)\n",
    "        subgroup_correct = np.zeros(num_subclasses)\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # print(labels)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            loss_train = criterion(outputs, labels)\n",
    "            tr_loss += loss_train.item()\n",
    "            test_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            \n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            \n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            test_accuracy = correct / total\n",
    "        \n",
    "        min_value = min(subgroup_accuracy)   # find the minimum value\n",
    "        min_index = subgroup_accuracy.index(min_value)  # find the index of the minimum value\n",
    "        \n",
    "        # Write the minimum value and its index to a CSV file\n",
    "        with open('minimum_values.csv', mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if file.tell() == 0:  # write header row if file is empty\n",
    "                writer.writerow(['Minimum Value', 'Index'])\n",
    "            writer.writerow([min_value, min_index])\n",
    "\n",
    "        print(\"Epoch: {:.3f}, Loss: {:.3f}, Test Accuracy: {:.3f}\".format(epoch+1, test_loss_value, test_accuracy)) \n",
    "        # print('confusion matrix of testing images: {}'.format(confusion_matrix))\n",
    "        # print(\"Test Accuracy:\", accuracy, \"\\nTest Accuracy over subgroups:\", subgroup_accuracy, \"\\nTest Worst Group Accuracy:\",\n",
    "        #       min(subgroup_accuracy)) \n",
    "   \n",
    "        return test_accuracy, subgroup_accuracy\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_11124\\4064068089.py:43: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.629, Train_Accuracy: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_11124\\2482126761.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.420, Val Accuracy: 0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_11124\\1403991071.py:47: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.586, Test Accuracy: 0.728\n",
      "Epoch: 2.000, Loss: 0.587, Train_Accuracy: 0.714\n",
      "Epoch: 2.000, Loss: 0.406, Val Accuracy: 0.741\n",
      "Epoch: 2.000, Loss: 0.558, Test Accuracy: 0.761\n",
      "Epoch: 3.000, Loss: 0.563, Train_Accuracy: 0.738\n",
      "Epoch: 3.000, Loss: 0.400, Val Accuracy: 0.747\n",
      "Epoch: 3.000, Loss: 0.541, Test Accuracy: 0.758\n",
      "Epoch: 4.000, Loss: 0.547, Train_Accuracy: 0.750\n",
      "Epoch: 4.000, Loss: 0.396, Val Accuracy: 0.744\n",
      "Epoch: 4.000, Loss: 0.530, Test Accuracy: 0.763\n",
      "Epoch: 5.000, Loss: 0.535, Train_Accuracy: 0.759\n",
      "Epoch: 5.000, Loss: 0.393, Val Accuracy: 0.746\n",
      "Epoch: 5.000, Loss: 0.522, Test Accuracy: 0.764\n",
      "Epoch: 6.000, Loss: 0.526, Train_Accuracy: 0.769\n",
      "Epoch: 6.000, Loss: 0.392, Val Accuracy: 0.748\n",
      "Epoch: 6.000, Loss: 0.516, Test Accuracy: 0.768\n",
      "Epoch: 7.000, Loss: 0.518, Train_Accuracy: 0.772\n",
      "Epoch: 7.000, Loss: 0.391, Val Accuracy: 0.749\n",
      "Epoch: 7.000, Loss: 0.511, Test Accuracy: 0.773\n",
      "Epoch: 8.000, Loss: 0.512, Train_Accuracy: 0.773\n",
      "Epoch: 8.000, Loss: 0.390, Val Accuracy: 0.753\n",
      "Epoch: 8.000, Loss: 0.507, Test Accuracy: 0.775\n",
      "Epoch: 9.000, Loss: 0.507, Train_Accuracy: 0.777\n",
      "Epoch: 9.000, Loss: 0.390, Val Accuracy: 0.757\n",
      "Epoch: 9.000, Loss: 0.504, Test Accuracy: 0.779\n",
      "Epoch: 10.000, Loss: 0.502, Train_Accuracy: 0.780\n",
      "Epoch: 10.000, Loss: 0.389, Val Accuracy: 0.760\n",
      "Epoch: 10.000, Loss: 0.501, Test Accuracy: 0.782\n",
      "Epoch: 11.000, Loss: 0.498, Train_Accuracy: 0.782\n",
      "Epoch: 11.000, Loss: 0.389, Val Accuracy: 0.763\n",
      "Epoch: 11.000, Loss: 0.499, Test Accuracy: 0.781\n",
      "Epoch: 12.000, Loss: 0.495, Train_Accuracy: 0.782\n",
      "Epoch: 12.000, Loss: 0.389, Val Accuracy: 0.768\n",
      "Epoch: 12.000, Loss: 0.497, Test Accuracy: 0.780\n",
      "Epoch: 13.000, Loss: 0.492, Train_Accuracy: 0.783\n",
      "Epoch: 13.000, Loss: 0.389, Val Accuracy: 0.770\n",
      "Epoch: 13.000, Loss: 0.495, Test Accuracy: 0.779\n",
      "Epoch: 14.000, Loss: 0.489, Train_Accuracy: 0.784\n",
      "Epoch: 14.000, Loss: 0.389, Val Accuracy: 0.772\n",
      "Epoch: 14.000, Loss: 0.494, Test Accuracy: 0.781\n",
      "Epoch: 15.000, Loss: 0.487, Train_Accuracy: 0.784\n",
      "Epoch: 15.000, Loss: 0.389, Val Accuracy: 0.772\n",
      "Epoch: 15.000, Loss: 0.493, Test Accuracy: 0.781\n",
      "Epoch: 16.000, Loss: 0.484, Train_Accuracy: 0.785\n",
      "Epoch: 16.000, Loss: 0.388, Val Accuracy: 0.772\n",
      "Epoch: 16.000, Loss: 0.491, Test Accuracy: 0.783\n",
      "Epoch: 17.000, Loss: 0.482, Train_Accuracy: 0.786\n",
      "Epoch: 17.000, Loss: 0.388, Val Accuracy: 0.771\n",
      "Epoch: 17.000, Loss: 0.490, Test Accuracy: 0.784\n",
      "Epoch: 18.000, Loss: 0.480, Train_Accuracy: 0.788\n",
      "Epoch: 18.000, Loss: 0.388, Val Accuracy: 0.771\n",
      "Epoch: 18.000, Loss: 0.489, Test Accuracy: 0.787\n",
      "Epoch: 19.000, Loss: 0.479, Train_Accuracy: 0.788\n",
      "Epoch: 19.000, Loss: 0.388, Val Accuracy: 0.772\n",
      "Epoch: 19.000, Loss: 0.488, Test Accuracy: 0.788\n",
      "Epoch: 20.000, Loss: 0.477, Train_Accuracy: 0.790\n",
      "Epoch: 20.000, Loss: 0.387, Val Accuracy: 0.773\n",
      "Epoch: 20.000, Loss: 0.487, Test Accuracy: 0.789\n",
      "Epoch: 21.000, Loss: 0.475, Train_Accuracy: 0.791\n",
      "Epoch: 21.000, Loss: 0.387, Val Accuracy: 0.773\n",
      "Epoch: 21.000, Loss: 0.486, Test Accuracy: 0.790\n",
      "Epoch: 22.000, Loss: 0.474, Train_Accuracy: 0.792\n",
      "Epoch: 22.000, Loss: 0.387, Val Accuracy: 0.772\n",
      "Epoch: 22.000, Loss: 0.485, Test Accuracy: 0.792\n",
      "Epoch: 23.000, Loss: 0.473, Train_Accuracy: 0.794\n",
      "Epoch: 23.000, Loss: 0.387, Val Accuracy: 0.772\n",
      "Epoch: 23.000, Loss: 0.484, Test Accuracy: 0.791\n",
      "Epoch: 24.000, Loss: 0.471, Train_Accuracy: 0.795\n",
      "Epoch: 24.000, Loss: 0.386, Val Accuracy: 0.772\n",
      "Epoch: 24.000, Loss: 0.484, Test Accuracy: 0.792\n",
      "Epoch: 25.000, Loss: 0.470, Train_Accuracy: 0.795\n",
      "Epoch: 25.000, Loss: 0.386, Val Accuracy: 0.772\n",
      "Epoch: 25.000, Loss: 0.483, Test Accuracy: 0.794\n",
      "Epoch: 26.000, Loss: 0.469, Train_Accuracy: 0.796\n",
      "Epoch: 26.000, Loss: 0.386, Val Accuracy: 0.771\n",
      "Epoch: 26.000, Loss: 0.482, Test Accuracy: 0.795\n",
      "Epoch: 27.000, Loss: 0.468, Train_Accuracy: 0.796\n",
      "Epoch: 27.000, Loss: 0.385, Val Accuracy: 0.773\n",
      "Epoch: 27.000, Loss: 0.481, Test Accuracy: 0.795\n",
      "Epoch: 28.000, Loss: 0.467, Train_Accuracy: 0.796\n",
      "Epoch: 28.000, Loss: 0.385, Val Accuracy: 0.772\n",
      "Epoch: 28.000, Loss: 0.481, Test Accuracy: 0.796\n",
      "Epoch: 29.000, Loss: 0.466, Train_Accuracy: 0.797\n",
      "Epoch: 29.000, Loss: 0.384, Val Accuracy: 0.774\n",
      "Epoch: 29.000, Loss: 0.480, Test Accuracy: 0.796\n",
      "Epoch: 30.000, Loss: 0.465, Train_Accuracy: 0.796\n",
      "Epoch: 30.000, Loss: 0.384, Val Accuracy: 0.774\n",
      "Epoch: 30.000, Loss: 0.479, Test Accuracy: 0.797\n",
      "Epoch: 31.000, Loss: 0.464, Train_Accuracy: 0.796\n",
      "Epoch: 31.000, Loss: 0.383, Val Accuracy: 0.774\n",
      "Epoch: 31.000, Loss: 0.479, Test Accuracy: 0.799\n",
      "Epoch: 32.000, Loss: 0.463, Train_Accuracy: 0.797\n",
      "Epoch: 32.000, Loss: 0.383, Val Accuracy: 0.777\n",
      "Epoch: 32.000, Loss: 0.478, Test Accuracy: 0.798\n",
      "Epoch: 33.000, Loss: 0.462, Train_Accuracy: 0.796\n",
      "Epoch: 33.000, Loss: 0.383, Val Accuracy: 0.777\n",
      "Epoch: 33.000, Loss: 0.477, Test Accuracy: 0.797\n",
      "Epoch: 34.000, Loss: 0.461, Train_Accuracy: 0.798\n",
      "Epoch: 34.000, Loss: 0.382, Val Accuracy: 0.777\n",
      "Epoch: 34.000, Loss: 0.477, Test Accuracy: 0.797\n",
      "Epoch: 35.000, Loss: 0.460, Train_Accuracy: 0.797\n",
      "Epoch: 35.000, Loss: 0.382, Val Accuracy: 0.777\n",
      "Epoch: 35.000, Loss: 0.476, Test Accuracy: 0.797\n",
      "Epoch: 36.000, Loss: 0.459, Train_Accuracy: 0.798\n",
      "Epoch: 36.000, Loss: 0.381, Val Accuracy: 0.777\n",
      "Epoch: 36.000, Loss: 0.476, Test Accuracy: 0.797\n",
      "Epoch: 37.000, Loss: 0.458, Train_Accuracy: 0.798\n",
      "Epoch: 37.000, Loss: 0.381, Val Accuracy: 0.778\n",
      "Epoch: 37.000, Loss: 0.475, Test Accuracy: 0.796\n",
      "Epoch: 38.000, Loss: 0.458, Train_Accuracy: 0.798\n",
      "Epoch: 38.000, Loss: 0.380, Val Accuracy: 0.780\n",
      "Epoch: 38.000, Loss: 0.474, Test Accuracy: 0.798\n",
      "Epoch: 39.000, Loss: 0.457, Train_Accuracy: 0.798\n",
      "Epoch: 39.000, Loss: 0.380, Val Accuracy: 0.781\n",
      "Epoch: 39.000, Loss: 0.474, Test Accuracy: 0.798\n",
      "Epoch: 40.000, Loss: 0.456, Train_Accuracy: 0.799\n",
      "Epoch: 40.000, Loss: 0.379, Val Accuracy: 0.782\n",
      "Epoch: 40.000, Loss: 0.473, Test Accuracy: 0.798\n",
      "Epoch: 41.000, Loss: 0.455, Train_Accuracy: 0.799\n",
      "Epoch: 41.000, Loss: 0.379, Val Accuracy: 0.783\n",
      "Epoch: 41.000, Loss: 0.473, Test Accuracy: 0.799\n",
      "Epoch: 42.000, Loss: 0.455, Train_Accuracy: 0.798\n",
      "Epoch: 42.000, Loss: 0.378, Val Accuracy: 0.782\n",
      "Epoch: 42.000, Loss: 0.472, Test Accuracy: 0.799\n",
      "Epoch: 43.000, Loss: 0.454, Train_Accuracy: 0.798\n",
      "Epoch: 43.000, Loss: 0.378, Val Accuracy: 0.782\n",
      "Epoch: 43.000, Loss: 0.472, Test Accuracy: 0.799\n",
      "Epoch: 44.000, Loss: 0.453, Train_Accuracy: 0.799\n",
      "Epoch: 44.000, Loss: 0.377, Val Accuracy: 0.783\n",
      "Epoch: 44.000, Loss: 0.471, Test Accuracy: 0.798\n",
      "Epoch: 45.000, Loss: 0.453, Train_Accuracy: 0.799\n",
      "Epoch: 45.000, Loss: 0.377, Val Accuracy: 0.782\n",
      "Epoch: 45.000, Loss: 0.471, Test Accuracy: 0.797\n",
      "Epoch: 46.000, Loss: 0.452, Train_Accuracy: 0.799\n",
      "Epoch: 46.000, Loss: 0.377, Val Accuracy: 0.783\n",
      "Epoch: 46.000, Loss: 0.470, Test Accuracy: 0.797\n",
      "Epoch: 47.000, Loss: 0.451, Train_Accuracy: 0.799\n",
      "Epoch: 47.000, Loss: 0.376, Val Accuracy: 0.782\n",
      "Epoch: 47.000, Loss: 0.470, Test Accuracy: 0.798\n",
      "Epoch: 48.000, Loss: 0.451, Train_Accuracy: 0.799\n",
      "Epoch: 48.000, Loss: 0.376, Val Accuracy: 0.782\n",
      "Epoch: 48.000, Loss: 0.469, Test Accuracy: 0.798\n",
      "Epoch: 49.000, Loss: 0.450, Train_Accuracy: 0.800\n",
      "Epoch: 49.000, Loss: 0.375, Val Accuracy: 0.783\n",
      "Epoch: 49.000, Loss: 0.469, Test Accuracy: 0.798\n",
      "Epoch: 50.000, Loss: 0.450, Train_Accuracy: 0.800\n",
      "Epoch: 50.000, Loss: 0.375, Val Accuracy: 0.783\n",
      "Epoch: 50.000, Loss: 0.468, Test Accuracy: 0.798\n",
      "Epoch: 51.000, Loss: 0.449, Train_Accuracy: 0.800\n",
      "Epoch: 51.000, Loss: 0.374, Val Accuracy: 0.782\n",
      "Epoch: 51.000, Loss: 0.468, Test Accuracy: 0.798\n",
      "Epoch: 52.000, Loss: 0.448, Train_Accuracy: 0.801\n",
      "Epoch: 52.000, Loss: 0.374, Val Accuracy: 0.782\n",
      "Epoch: 52.000, Loss: 0.467, Test Accuracy: 0.797\n",
      "Epoch: 53.000, Loss: 0.448, Train_Accuracy: 0.801\n",
      "Epoch: 53.000, Loss: 0.373, Val Accuracy: 0.783\n",
      "Epoch: 53.000, Loss: 0.467, Test Accuracy: 0.798\n",
      "Epoch: 54.000, Loss: 0.447, Train_Accuracy: 0.802\n",
      "Epoch: 54.000, Loss: 0.373, Val Accuracy: 0.782\n",
      "Epoch: 54.000, Loss: 0.466, Test Accuracy: 0.799\n",
      "Epoch: 55.000, Loss: 0.447, Train_Accuracy: 0.802\n",
      "Epoch: 55.000, Loss: 0.373, Val Accuracy: 0.782\n",
      "Epoch: 55.000, Loss: 0.466, Test Accuracy: 0.799\n",
      "Epoch: 56.000, Loss: 0.446, Train_Accuracy: 0.802\n",
      "Epoch: 56.000, Loss: 0.372, Val Accuracy: 0.781\n",
      "Epoch: 56.000, Loss: 0.466, Test Accuracy: 0.799\n",
      "Epoch: 57.000, Loss: 0.446, Train_Accuracy: 0.802\n",
      "Epoch: 57.000, Loss: 0.372, Val Accuracy: 0.781\n",
      "Epoch: 57.000, Loss: 0.465, Test Accuracy: 0.800\n",
      "Epoch: 58.000, Loss: 0.445, Train_Accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "# number of epochs\n",
    "n_epochs = 68\n",
    "tr_loss = []\n",
    "tr_accuracies = []\n",
    "val_loss = []\n",
    "val_accuracies = []\n",
    "test_loss = []\n",
    "test_accuracies = []\n",
    "max_worst_accuracy = 0\n",
    "\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "num_trials = 30\n",
    "df1 = pd.DataFrame(columns=['trial', 'subtype', 'Train ERM accuracy'])\n",
    "df2 = pd.DataFrame(columns=['trial', 'subtype', 'Val ERM accuracy'])\n",
    "df3 = pd.DataFrame(columns=['trial', 'subtype', 'Test ERM accuracy'])\n",
    "subgroups = {'adenosis',\n",
    "            'fibroadenoma',\n",
    "            'tubular_adenoma',\n",
    "            'phyllodes_tumor',\n",
    "            'ductal_carcinoma',\n",
    "            'lobular_carcinoma',\n",
    "            'mucinous_carcinoma',\n",
    "            'papillary_carcinoma'}\n",
    "\n",
    "# training the model\n",
    "for i in range(num_trials):\n",
    "    for epoch in range(n_epochs):\n",
    "        temptrain_acc, trainsubgroup_acc = train(epoch)\n",
    "        tempval_acc, valsubgroup_acc = val(epoch)\n",
    "        temptest_acc, testsubgroup_acc = test(epoch)\n",
    "        \n",
    "    # append accuracy values for each subgroup to the dataframes\n",
    "    for j, acc in zip(subgroups, trainsubgroup_acc):\n",
    "        df1 = df1.append({'trial': i, 'subtype': j, 'Train ERM accuracy': acc}, ignore_index=True)\n",
    "    \n",
    "    # add overall accuracy to Train gDRO accuracy for each trial\n",
    "    df1 = df1.append({'trial': i, 'subtype': 'overall', 'Train ERM accuracy': temptrain_acc}, ignore_index=True)\n",
    "    \n",
    "    for j, acc in zip(subgroups, valsubgroup_acc):\n",
    "        df2 = df2.append({'trial': i, 'subtype': j, 'Val ERM accuracy': acc}, ignore_index=True)\n",
    "    \n",
    "    # add overall accuracy to Val gDRO accuracy for each trial\n",
    "    df2 = df2.append({'trial': i, 'subtype': 'overall', 'Val ERM accuracy': tempval_acc}, ignore_index=True)\n",
    "    \n",
    "    for j, acc in zip(subgroups, testsubgroup_acc):\n",
    "        df3 = df3.append({'trial': i, 'subtype': j, 'Test ERM accuracy': acc}, ignore_index=True)\n",
    "        \n",
    "    # add overall accuracy to Test gDRO accuracy for each trial\n",
    "    df3 = df3.append({'trial': i, 'subtype': 'overall', 'Test ERM accuracy': temptest_acc}, ignore_index=True)\n",
    "\n",
    "        # Check if the validation loss has increased and update the best model if it hasn't\n",
    "        # a =  min(valsubgroup_acc)\n",
    "        # print(a)\n",
    "        # if a >= max_worst_accuracy:\n",
    "        #     max_worst_accuracy = a\n",
    "        #     best_epoch = epoch\n",
    "        #     print('I am saving the best dro model at Epoch: ', best_epoch)\n",
    "        #     torch.save(model.state_dict(), r'Best_gdromodel.pth')\n",
    "        #     counter = 0\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     if counter >= patience: # Stop early if the validation loss has increased for `patience` epochs\n",
    "        #         break\n",
    "     \n",
    "    # print(\"For Trial:\",i,\"Train Accuracy:\", temptrain_acc, \"\\nTrain Accuracy over each subgroups:\", trainsubgroup_acc, \"\\nTrain Worst Group Accuracy:\",min(trainsubgroup_acc))\n",
    "    # print(\"For Trial:\",i,\"Val Accuracy:\", tempval_acc, \"\\nVal Accuracy over each subgroups:\", valsubgroup_acc, \"\\nVal Worst Group Accuracy:\",min(valsubgroup_acc))\n",
    "    print(\"For Trial:\",i,\"Test Accuracy:\", temptest_acc,\"\\nTest Accuracy over each subgroups:\", testsubgroup_acc, \"\\ntest Worst Group Accuracy:\",min(testsubgroup_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that df1 is your data frame\n",
    "df1.to_csv('Train_bc-erm_radnet.csv', index=False)\n",
    "df2.to_csv('Val_bc-erm_radnet.csv', index=False)\n",
    "df3.to_csv('Test_bc-erm_radnet.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
