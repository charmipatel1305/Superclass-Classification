{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import Linear, ReLU, BCEWithLogitsLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, Dropout\n",
    "from torch.optim import Adam,SGD\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler,Subset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA Version is 11.3\n",
      "Whether CUDA is supported by our system: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch CUDA Version is\", torch.version.cuda)\n",
    "print(\"Whether CUDA is supported by our system:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.data.to_string(index=False)\n",
    "#         self.transform = transform\n",
    "#         self.data['Super Class'] = self.data['Super Class'].apply(lambda x: 0 if x == 'benign' else 1)\n",
    "#         label_map = {\n",
    "#             'adenosis': 0,\n",
    "#             'fibroadenoma': 1,\n",
    "#             'tubular_adenoma': 2,\n",
    "#             'phyllodes_tumor': 3,\n",
    "#             'ductal_carcinoma': 4,\n",
    "#             'lobular_carcinoma': 5,\n",
    "#             'mucinous_carcinoma': 6,\n",
    "#             'papillary_carcinoma': 7\n",
    "#         }\n",
    "#         self.data['Sub Class'] = self.data['Sub Class'].map(label_map)\n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_path = self.data.iloc[idx, 0]\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         SuperClass = self.data.iloc[idx, 1]\n",
    "#         # SuperClass = torch.tensor(SuperClass, dtype=torch.long)\n",
    "#         SubClass = self.data.iloc[idx, 3]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         image = asarray(image)\n",
    "#         return (image, SuperClass, SubClass)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256,256)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# dataset = ImageDataset(csv_file='csv windows.csv', transform=transform)\n",
    "# images = [x[0] for x in dataset]\n",
    "# superclass_labels = [x[1] for x in dataset]\n",
    "# subclass_labels = [x[2] for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Serialize and save the list to a file\n",
    "# with open('image_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(images, f)\n",
    "\n",
    "# with open('superclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(superclass_labels, f)\n",
    "    \n",
    "\n",
    "# with open('subclass_labels.pkl', 'wb') as f:\n",
    "#     pickle.dump(subclass_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list back into memory\n",
    "file_pathimages = '/home/cpatel/superclass/data/images/images/image_list.pkl'\n",
    "\n",
    "with open(file_pathimages, 'rb') as f:\n",
    "    images = pickle.load(f)\n",
    "    \n",
    "with open('superclass_labels.pkl', 'rb') as f:\n",
    "    superclass_labels = pickle.load(f)\n",
    "\n",
    "with open('subclass_labels.pkl', 'rb') as f:\n",
    "    subclass_labels = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(images, superclass_labels, subclass_labels, test_size=0.3, stratify=superclass_labels, random_state=42)\n",
    "x_train, x_val, y_train, y_val, z_train, z_val = train_test_split(x_train, y_train, z_train, test_size=0.3, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Train=226, Val=107, Test=111\n",
      "Class 1: Train=484, Val=211, Test=319\n",
      "Class 2: Train=282, Val=111, Test=176\n",
      "Class 3: Train=223, Val=92, Test=138\n",
      "Class 4: Train=1700, Val=737, Test=1014\n",
      "Class 5: Train=284, Val=128, Test=214\n",
      "Class 6: Train=408, Val=175, Test=209\n",
      "Class 7: Train=268, Val=100, Test=192\n"
     ]
    }
   ],
   "source": [
    "# get the unique labels in y\n",
    "unique_labels = np.unique(subclass_labels)\n",
    "\n",
    "# count the number of samples in each set for each class\n",
    "train_counts = [np.sum(z_train == label) for label in unique_labels]\n",
    "val_counts = [np.sum(z_val == label) for label in unique_labels]\n",
    "test_counts = [np.sum(z_test == label) for label in unique_labels]\n",
    "\n",
    "# print the counts for each class in each set\n",
    "for i, label in enumerate(unique_labels): \n",
    "    print(f\"Class {label}: Train={train_counts[i]}, Val={val_counts[i]}, Test={test_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.z[index]\n",
    "\n",
    "train_dataset = YourDataset(x_train, y_train, z_train)\n",
    "val_dataset = YourDataset(x_val, y_val, z_val)\n",
    "test_dataset = YourDataset(x_test, y_test, z_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet50(weights=True)\n",
    "\n",
    "lt=10\n",
    "cntr = 0\n",
    "for child in model.children():\n",
    "    cntr+=1\n",
    "\n",
    "    if cntr < lt:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features = num_ftrs, out_features = 1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and loss function\n",
    "\n",
    "# define cnn model\n",
    "model = model.to(device)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# define optimizer\n",
    "optimizer = Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# define loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train(epoch):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    tr_loss=0\n",
    "    model.train()\n",
    "    \n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, subclass = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        # print(labels)\n",
    "        # print(subclass)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # predcition for training set\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        \n",
    "        outputs = outputs.flatten()\n",
    "        total += labels.size(0)\n",
    "        # print(total)\n",
    "        \n",
    "        y_2 = torch.zeros(len(outputs))\n",
    "        y_2[outputs>=0.0] = 1\n",
    "        y_2= y_2.int()\n",
    "        y_2 = y_2.to(device)\n",
    "        \n",
    "        for subclasses in range(num_subclasses):\n",
    "            subclass_idx = subclass == subclasses\n",
    "            num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "            subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                torch.float).sum().item()\n",
    "\n",
    "        subgroup_accuracy = subgroup_correct / num_samples\n",
    "        # print(y_2)\n",
    "        correct += (y_2 == labels).sum().item()\n",
    "        train_accuracy = correct / total \n",
    "        # for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "        #         confusion_matrix[t.long(), p.long()] += 1\n",
    "        \n",
    "        # clearing the gradients of the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute loss\n",
    "        loss_train = criterion(outputs, labels)\n",
    "        # print(loss_train)\n",
    "        \n",
    "        # compute updates weights of all the parameters\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss_train.item()\n",
    "\n",
    "    # Calculate training loss value\n",
    "    train_loss_value = tr_loss/len(train_loader) \n",
    "    print(\"Epoch: {:.3f}, Loss: {:.3f}, Train_Accuracy: {:.3f}\".format(epoch+1, train_loss_value, train_accuracy)) \n",
    "    # print('confusion matrix of training images: {}'.format(confusion_matrix))\n",
    "\n",
    "    # print(\"Train Accuracy:\", accuracy, \"\\nTrain Accuracy over subgroups:\", subgroup_accuracy, \"\\nTrain Worst Group Accuracy:\",\n",
    "    #           min(subgroup_accuracy))\n",
    "    \n",
    "    return train_accuracy, subgroup_accuracy, train_loss_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    num_subclasses = 8\n",
    "    num_samples = np.zeros(num_subclasses)\n",
    "    subgroup_correct = np.zeros(num_subclasses)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            # print(labels)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            loss_train = criterion(outputs, labels)\n",
    "            tr_loss += loss_train.item()\n",
    "            val_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            \n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            val_accuracy = correct / total\n",
    "            \n",
    "        print(\"Epoch: {:.3f}, Loss: {:.3f}, Val Accuracy: {:.3f}\".format(epoch+1, val_loss_value, val_accuracy)) \n",
    "        # print('confusion matrix of validation images: {}'.format(confusion_matrix)) \n",
    "        # print(\"Val Accuracy:\", accuracy, \"\\nVal Accuracy over subgroups:\", subgroup_accuracy, \"\\nVal Worst Group Accuracy:\",\n",
    "        #       min(subgroup_accuracy))       \n",
    "    \n",
    "        return val_accuracy, subgroup_accuracy, val_loss_value\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    nb_classes = 2\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    subgroup_correct_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tr_loss = 0\n",
    "        num_subclasses = 8\n",
    "        num_samples = np.zeros(num_subclasses)\n",
    "        subgroup_correct = np.zeros(num_subclasses)\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels,subclass = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # print(labels)\n",
    "            \n",
    "            # predcition for training set\n",
    "            outputs = model(inputs)\n",
    "            # print(outputs)\n",
    "            \n",
    "            outputs = outputs.flatten()\n",
    "            total += labels.size(0)\n",
    "            # print(total)\n",
    "            \n",
    "            loss_train = criterion(outputs, labels)\n",
    "            tr_loss += loss_train.item()\n",
    "            test_loss_value = tr_loss/len(test_loader)\n",
    "            \n",
    "            y_2 = torch.zeros(len(outputs))\n",
    "            y_2[outputs>=0.0] = 1\n",
    "            y_2 = y_2.int()\n",
    "            y_2 = y_2.to(device)\n",
    "            \n",
    "            for subclasses in range(num_subclasses):\n",
    "                subclass_idx = subclass == subclasses\n",
    "                num_samples[subclasses] += torch.sum(subclass_idx)\n",
    "                subgroup_correct[subclasses] += (y_2[subclass_idx] == labels[subclass_idx]).type(\n",
    "                    torch.float).sum().item()\n",
    "\n",
    "            subgroup_accuracy = subgroup_correct / num_samples\n",
    "\n",
    "            \n",
    "            for t, p in zip(labels.view(-1), y_2.view(-1)):\n",
    "                    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            \n",
    "            correct += (y_2 == labels).sum().item()\n",
    "            test_accuracy = correct / total\n",
    "            \n",
    "        print(\"Epoch: {:.3f}, Loss: {:.3f}, Test Accuracy: {:.3f}\".format(epoch+1, test_loss_value, test_accuracy)) \n",
    "        # print('confusion matrix of testing images: {}'.format(confusion_matrix))\n",
    "        # print(\"Test Accuracy:\", accuracy, \"\\nTest Accuracy over subgroups:\", subgroup_accuracy, \"\\nTest Worst Group Accuracy:\",\n",
    "        #       min(subgroup_accuracy)) \n",
    "   \n",
    "        return test_accuracy, subgroup_accuracy, test_loss_value\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_30500\\4064068089.py:43: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.566, Train_Accuracy: 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_30500\\2482126761.py:46: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.363, Val Accuracy: 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpatel54\\AppData\\Local\\Temp\\ipykernel_30500\\262115633.py:48: RuntimeWarning: invalid value encountered in divide\n",
      "  subgroup_accuracy = subgroup_correct / num_samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1.000, Loss: 0.513, Test Accuracy: 0.814\n",
      "Epoch: 2.000, Loss: 0.475, Train_Accuracy: 0.797\n",
      "Epoch: 2.000, Loss: 0.317, Val Accuracy: 0.846\n",
      "Epoch: 2.000, Loss: 0.447, Test Accuracy: 0.856\n",
      "Epoch: 3.000, Loss: 0.424, Train_Accuracy: 0.835\n",
      "Epoch: 3.000, Loss: 0.289, Val Accuracy: 0.858\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'minimum_values.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\One Drive\\OneDrive - DePaul University\\Datasets\\BreaKHis_v1\\Code\\TransferLearning\\bc-erm_imagenet.ipynb Cell 16\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     temptrain_acc, trainsubgroup_acc \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     tempval_acc, valsubgroup_acc \u001b[39m=\u001b[39m val(epoch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     temptest_acc, testsubgroup_acc \u001b[39m=\u001b[39m test(epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# append accuracy values for each subgroup to the dataframes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, acc \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(subgroups, trainsubgroup_acc):\n",
      "\u001b[1;32md:\\One Drive\\OneDrive - DePaul University\\Datasets\\BreaKHis_v1\\Code\\TransferLearning\\bc-erm_imagenet.ipynb Cell 16\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m min_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin(subgroup_accuracy) \u001b[39m# find the index of the minimum value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Write the minimum value and its index to a CSV file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mminimum_values.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m, newline\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     writer \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mwriter(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/One%20Drive/OneDrive%20-%20DePaul%20University/Datasets/BreaKHis_v1/Code/TransferLearning/bc-erm_imagenet.ipynb#X21sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     writer\u001b[39m.\u001b[39mwriterow([\u001b[39m'\u001b[39m\u001b[39mMinimum Value\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIndex\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'minimum_values.csv'"
     ]
    }
   ],
   "source": [
    "# number of epochs\n",
    "n_epochs = 4\n",
    "tr_loss = []\n",
    "tr_accuracies = []\n",
    "val_loss = []\n",
    "val_accuracies = []\n",
    "test_loss = []\n",
    "test_accuracies = []\n",
    "max_worst_accuracy = 0\n",
    "\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "num_trials = 1\n",
    "df1 = pd.DataFrame(columns=['trial', 'epochs', 'subtype', 'Train ERM accuracy', 'Train ERM loss'])\n",
    "df2 = pd.DataFrame(columns=['trial', 'epochs', 'subtype', 'Val ERM accuracy', 'Val ERM loss'])\n",
    "df3 = pd.DataFrame(columns=['trial', 'epochs', 'subtype', 'Test ERM accuracy', 'Test ERM loss'])\n",
    "minmax_acc_test = pd.DataFrame(columns=['trial', 'epochs', 'Min_subtype', 'Min subclass accuracy', 'Max_subtype', 'Max subclass accuracy'])\n",
    "\n",
    "subgroups = ['adenosis',\n",
    "            'fibroadenoma',\n",
    "            'tubular_adenoma',\n",
    "            'phyllodes_tumor',\n",
    "            'ductal_carcinoma',\n",
    "            'lobular_carcinoma',\n",
    "            'mucinous_carcinoma',\n",
    "            'papillary_carcinoma']\n",
    "\n",
    "# training the model\n",
    "for i in range(num_trials):\n",
    "    for epoch in range(n_epochs):\n",
    "        temptrain_acc, trainsubgroup_acc, train_loss = train(epoch)\n",
    "        tempval_acc, valsubgroup_acc, val_loss = val(epoch)\n",
    "        temptest_acc, testsubgroup_acc, test_loss = test(epoch)\n",
    "        \n",
    "        test_accuracies.append(temptest_acc)\n",
    "\n",
    "        \n",
    "        min_val = min(testsubgroup_acc)\n",
    "        min_index = testsubgroup_acc.argmin()\n",
    "        max_val = max(testsubgroup_acc)\n",
    "        max_index = testsubgroup_acc.argmax()\n",
    "        minmax_acc_test = minmax_acc_test.append({'trial': i, 'epochs': epoch, 'Min_subtype': min_index, 'Min subclass accuracy': min_val, 'Max_subtype': max_index, 'Max subclass accuracy': max_val}, ignore_index=True)\n",
    "        \n",
    "        # append accuracy values for each subgroup to the dataframes\n",
    "        for j, acc in zip(subgroups, trainsubgroup_acc):\n",
    "            df1 = df1.append({'trial': i, 'epochs': epoch, 'subtype': j, 'Train ERM accuracy': acc, 'Train ERM loss': train_loss}, ignore_index=True)\n",
    "        \n",
    "        # add overall accuracy to Train gDRO accuracy for each trial\n",
    "        df1 = df1.append({'trial': i, 'epochs': epoch, 'subtype': 'overall', 'Train ERM accuracy': temptrain_acc, 'Train ERM loss': train_loss}, ignore_index=True)\n",
    "        \n",
    "        for j, acc in zip(subgroups, valsubgroup_acc):\n",
    "            df2 = df2.append({'trial': i , 'epochs': epoch, 'subtype': j, 'Val ERM accuracy': acc, 'Val ERM loss': val_loss}, ignore_index=True)\n",
    "        \n",
    "        # add overall accuracy to Val gDRO accuracy for each trial\n",
    "        df2 = df2.append({'trial': i, 'epochs': epoch, 'subtype': 'overall', 'Val ERM accuracy': tempval_acc, 'Val ERM loss': val_loss}, ignore_index=True)\n",
    "        \n",
    "        for j, acc in zip(subgroups, testsubgroup_acc):\n",
    "            df3 = df3.append({'trial': i , 'epochs': epoch, 'subtype': j, 'Test ERM accuracy': acc, 'Test ERM loss': test_loss,},  ignore_index=True)\n",
    "            \n",
    "        # add overall accuracy to Test gDRO accuracy for each trial\n",
    "        df3 = df3.append({'trial': i,'epochs': epoch, 'subtype': 'overall', 'Test ERM accuracy': temptest_acc, 'Test ERM loss': test_loss}, ignore_index=True)\n",
    "\n",
    "        # Check if the validation loss has increased and update the best model if it hasn't\n",
    "        # a =  min(valsubgroup_acc)\n",
    "        # print(a)\n",
    "        # if a >= max_worst_accuracy:\n",
    "        #     max_worst_accuracy = a\n",
    "        #     best_epoch = epoch\n",
    "        #     print('I am saving the best dro model at Epoch: ', best_epoch)\n",
    "        #     torch.save(model.state_dict(), r'Best_gdromodel.pth')\n",
    "        #     counter = 0\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     if counter >= patience: # Stop early if the validation loss has increased for `patience` epochs\n",
    "        #         break\n",
    "     \n",
    "    # print(\"For Trial:\",i,\"Train Accuracy:\", temptrain_acc, \"\\nTrain Accuracy over each subgroups:\", trainsubgroup_acc, \"\\nTrain Worst Group Accuracy:\",min(trainsubgroup_acc))\n",
    "    # print(\"For Trial:\",i,\"Val Accuracy:\", tempval_acc, \"\\nVal Accuracy over each subgroups:\", valsubgroup_acc, \"\\nVal Worst Group Accuracy:\",min(valsubgroup_acc))\n",
    "    print(\"For Trial:\",i,\"Test Accuracy:\", temptest_acc,\"\\nTest Accuracy over each subgroups:\", testsubgroup_acc, \"\\ntest Worst Group Accuracy:\",min(testsubgroup_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming that df1 is your data frame\n",
    "df1.to_csv('Train_bc-erm_imagenet.csv', index=False)\n",
    "df2.to_csv('Val_bc-erm_imagenet.csv', index=False)\n",
    "df3.to_csv('Test_bc-erm_imagenet.csv', index=False)\n",
    "minmax_acc_test.to_csv('MinMax_acc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
